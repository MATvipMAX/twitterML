{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/final_replace_depress0220.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 73\u001b[0m\n\u001b[0;32m     69\u001b[0m NB_DEPRESS \u001b[39m=\u001b[39m \u001b[39m1984\u001b[39m\n\u001b[0;32m     71\u001b[0m DIR \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m../data/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> 73\u001b[0m inputs, labels \u001b[39m=\u001b[39m load_users(DIR, POST_SIZE, MAX_POSTS, NB_DEPRESS, \u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     74\u001b[0m control \u001b[39m=\u001b[39m inputs[inputs\u001b[39m.\u001b[39mlabel \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m     75\u001b[0m depress \u001b[39m=\u001b[39m inputs[inputs\u001b[39m.\u001b[39mlabel \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\mateu\\OneDrive\\Pulpit\\KCL\\Year 3\\Final_project\\PhDcodes\\test\\utils2\\users.py:9\u001b[0m, in \u001b[0;36mload_users\u001b[1;34m(_dir, post_size, max_posts, nb_depress, return_df)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_users\u001b[39m(_dir, post_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, max_posts\u001b[39m=\u001b[39m\u001b[39m2000\u001b[39m, nb_depress\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m, return_df\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m----> 9\u001b[0m     depress \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(_dir \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mfinal_replace_depress0220.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     10\u001b[0m     control \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mfinal_replace_control0220.csv\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m     \u001b[39m# depress = pd.read_csv(_dir + 'final_replace_depress0220token.csv', encoding='utf-8')\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[39m# control = pd.read_csv(_dir + 'final_replace_control0220token.csv', encoding='utf-8')\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    861\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(handle, ioargs\u001b[39m.\u001b[39;49mmode)\n\u001b[0;32m    866\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n\u001b[0;32m    868\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/final_replace_depress0220.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.random import seed\n",
    "from utils2 import split_data\n",
    "from utils2.tokeniser import Tokenizer\n",
    "from utils2.utils import get_class_weights\n",
    "from utils2.embed import load_glove_tw_vectors\n",
    "from utils2.users import load_users, select_post, combine_users\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from keras.optimizers import Adam#, AdaMod\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "#from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers import Embedding, Dropout, Input\n",
    "from keras.models import Model\n",
    "#from tensorflow import set_random_seed\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def create_dl_model():\n",
    "    num_filters = 100\n",
    "    drop = 0.5\n",
    "\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                input_length=INPUT_LENGTH,\n",
    "                                weights=[embedding_matrix],\n",
    "                                trainable=False)\n",
    "                                # trainable=True)\n",
    "\n",
    "    model_input = Input(shape=(INPUT_LENGTH,), dtype='int32')\n",
    "    model_layer = embedding_layer(model_input)\n",
    "    model_layer = Dropout(drop)(model_layer)\n",
    "    model_layer = Conv1D(filters=num_filters, kernel_size=3)(model_layer)\n",
    "    model_layer = GlobalMaxPooling1D()(model_layer)\n",
    "    model_layer = Dropout(drop)(model_layer)\n",
    "    model_layer = Dense(100, activation='relu')(model_layer)\n",
    "    model_layer = Dropout(drop)(model_layer)\n",
    "    model_layer = Dense(2, activation='softmax')(model_layer)\n",
    "    model = Model(model_input, model_layer)\n",
    "\n",
    "    print(model.summary())\n",
    "    adam = Adam(lr=0.007, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    # adam = AdaMod(lr=0.007)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])  # metrics.binary_accuracy\n",
    "    return model\n",
    "\n",
    "\n",
    "SEED = 1\n",
    "seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "#set_random_seed(SEED)\n",
    "\n",
    "POST_SIZE = 100\n",
    "MAX_POSTS = 3200\n",
    "MAX_NB_WORDS = 50000\n",
    "INPUT_LENGTH = 35000\n",
    "EMBEDDING_DIM = 100\n",
    "NB_DEPRESS = 1984\n",
    "\n",
    "DIR = '../data/'\n",
    "\n",
    "inputs, labels = load_users(DIR, POST_SIZE, MAX_POSTS, NB_DEPRESS, True)\n",
    "control = inputs[inputs.label == 0].reset_index(drop=True).copy()\n",
    "depress = inputs[inputs.label == 1].reset_index(drop=True).copy()\n",
    "del inputs, labels\n",
    "\n",
    "embeddings_index = load_glove_tw_vectors(EMBEDDING_DIM)  # load pre-trained embedding vectors\n",
    "\n",
    "i = 0\n",
    "MODEL_FOLDER = \"logs/{}/{}\".format('DL', datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M\") + '-' + str(MAX_NB_WORDS) + '-' + str(INPUT_LENGTH))\n",
    "for data_index in split_data.split(depress, 4):\n",
    "    data_fold = depress[depress.userid.isin(data_index)].copy()\n",
    "\n",
    "    print(data_index[:10])\n",
    "    print(data_fold.head())\n",
    "\n",
    "    combine, labels = combine_users(control, data_fold)\n",
    "    inputs, labels = select_post(combine, labels, MAX_POSTS)\n",
    "#     del combine, data_fold\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of control:  1699\n",
      "The number of depress:  1984\n",
      "Control shape:  (3944945, 3)\n",
      "Depress shape:  (4996621, 3)\n",
      "Selecting users with at least 100 posts\n",
      "Control users:  1699\n",
      "-----------12.831624031066895-----------\n",
      "Depress users:  1984\n",
      "-----------17.756147623062134-----------\n",
      "Labelling\n",
      "Loading embedding vectors from a file\n",
      "Total 1193514 word vectors.\n",
      "[ 780078428687065089            25567061           416684112\n",
      "           608725707  912541969968066560          2700680921\n",
      "          2329059770 1064934140007538688            43903078\n",
      "            16656127]\n",
      "                  tweetid   userid  \\\n",
      "3234  1143666788107866112  5882452   \n",
      "3235  1143666924246560768  5882452   \n",
      "3236  1143664692679720961  5882452   \n",
      "3237  1143661172891066370  5882452   \n",
      "3238  1143669737378893824  5882452   \n",
      "\n",
      "                                                  clean  label  \n",
      "3234  user user i've always felt like i was kind of ...      1  \n",
      "3235  user user i cannot function without my morning...      1  \n",
      "3236  user way too much of what you're describing so...      1  \n",
      "3237  user between chronic pain, anxiety, and depres...      1  \n",
      "3238  user user benadryl stops my itching but does n...      1  \n",
      "Selecting 3200 posts from each user\n",
      "-----------16.80888605117798-----------\n",
      "0    1699\n",
      "1     496\n",
      "Name: label, dtype: int64\n",
      "Input number:  2195\n",
      "Label number:  2195\n",
      "Tokenizing text\n",
      "Total 807701 unique tokens.\n",
      "-----------226.6247479915619-----------\n",
      "Transforming text to input sequences\n",
      "-----------411.7837245464325-----------\n",
      "Finished loading data\n",
      "Shape of data tensor: (2195, 35000)\n",
      "Shape of label tensor: (2195, 2)\n",
      "Building embedding matrix\n",
      "50000 Selected word tokens\n",
      "-----------0.08873367309570312-----------\n",
      "--------------Start training----------------2020-02-19 21:50:57.417593\n",
      "Number of positive and negative classes in training and validation set\n",
      "[1359.  397.]\n",
      "[340.  99.]\n",
      "model fitting - Hierachical LSTM\n",
      "{0: 1.0, 1: 3.4231738035264483}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 35000)             0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 35000, 100)        5000000   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 35000, 100)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 34998, 250)        75250     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 502       \n",
      "=================================================================\n",
      "Total params: 5,138,502\n",
      "Trainable params: 138,502\n",
      "Non-trainable params: 5,000,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\ipykernel_launcher.py:200: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Finished training----------------\n",
      "[         2826065615          3960721393          4318344515\n",
      "          3382208403          2599342730          2192916252\n",
      "           181892265 1091181448843931649  836044275712475136\n",
      "            60688549]\n",
      "                  tweetid   userid  \\\n",
      "6479  1117414019164454912  6111852   \n",
      "6480  1117414158188929024  6111852   \n",
      "6481  1117413476631220226  6111852   \n",
      "6482  1116951765906227200  6111852   \n",
      "6483  1117037694268342272  6111852   \n",
      "\n",
      "                                                  clean  label  \n",
      "6479                     good morning, twitterverse url      1  \n",
      "6480  rt user: everything you need to know about k-p...      1  \n",
      "6481  it's also a rhetorical trick thwimp uses all t...      1  \n",
      "6482                          rt user: this thread: url      1  \n",
      "6483  putin has said america is no longer a leader. ...      1  \n",
      "Selecting 3200 posts from each user\n",
      "-----------17.453445196151733-----------\n",
      "0    1699\n",
      "1     496\n",
      "Name: label, dtype: int64\n",
      "Input number:  2195\n",
      "Label number:  2195\n",
      "Tokenizing text\n",
      "Total 804337 unique tokens.\n",
      "-----------224.37322640419006-----------\n",
      "Transforming text to input sequences\n",
      "-----------401.7037615776062-----------\n",
      "Finished loading data\n",
      "Shape of data tensor: (2195, 35000)\n",
      "Shape of label tensor: (2195, 2)\n",
      "Building embedding matrix\n",
      "50000 Selected word tokens\n",
      "-----------0.13772058486938477-----------\n",
      "--------------Start training----------------2020-02-19 22:50:20.849460\n",
      "Number of positive and negative classes in training and validation set\n",
      "[1359.  397.]\n",
      "[340.  99.]\n",
      "model fitting - Hierachical LSTM\n",
      "{0: 1.0, 1: 3.4231738035264483}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 35000)             0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 35000, 100)        5000000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 35000, 100)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 34998, 250)        75250     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 502       \n",
      "=================================================================\n",
      "Total params: 5,138,502\n",
      "Trainable params: 138,502\n",
      "Non-trainable params: 5,000,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\ipykernel_launcher.py:200: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Finished training----------------\n",
      "[           22936389          1586758399          3227968831\n",
      "          2731028615          3056177975            25787341\n",
      " 1104780414516977666           280932608            56891919\n",
      "  800940814641856512]\n",
      "               tweetid  userid  \\\n",
      "0   589962698353299457  802310   \n",
      "1  1130666193289863168  802310   \n",
      "2  1070479497898520577  802310   \n",
      "3  1070482628153794560  802310   \n",
      "4  1070483237879799809  802310   \n",
      "\n",
      "                                               clean  label  \n",
      "0  user github is giving me more grief than the l...      1  \n",
      "1  user some of us just ride the risk-train and g...      1  \n",
      "2  user super happy to see another competent deve...      1  \n",
      "3  user you can't trust lnp to make sensible cybe...      1  \n",
      "4  user user is user prepared to stand against <h...      1  \n",
      "Selecting 3200 posts from each user\n",
      "-----------15.92917799949646-----------\n",
      "0    1699\n",
      "1     496\n",
      "Name: label, dtype: int64\n",
      "Input number:  2195\n",
      "Label number:  2195\n",
      "Tokenizing text\n",
      "Total 793449 unique tokens.\n",
      "-----------224.05191326141357-----------\n",
      "Transforming text to input sequences\n",
      "-----------405.8782205581665-----------\n",
      "Finished loading data\n",
      "Shape of data tensor: (2195, 35000)\n",
      "Shape of label tensor: (2195, 2)\n",
      "Building embedding matrix\n",
      "50000 Selected word tokens\n",
      "-----------0.14162206649780273-----------\n",
      "--------------Start training----------------2020-02-19 23:50:21.837062\n",
      "Number of positive and negative classes in training and validation set\n",
      "[1359.  397.]\n",
      "[340.  99.]\n",
      "model fitting - Hierachical LSTM\n",
      "{0: 1.0, 1: 3.4231738035264483}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 35000)             0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 35000, 100)        5000000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 35000, 100)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 34998, 250)        75250     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 502       \n",
      "=================================================================\n",
      "Total params: 5,138,502\n",
      "Trainable params: 138,502\n",
      "Non-trainable params: 5,000,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\ipykernel_launcher.py:200: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Finished training----------------\n",
      "[ 960112188433338368 1004044855931293696           497519459\n",
      "            41555588  753773198383128576          2868511826\n",
      " 1019322256953106437  962874724987551744 1004689374221623297\n",
      "          2237542326]\n",
      "                   tweetid    userid  \\\n",
      "22499  1089551448323575809  13379712   \n",
      "22500  1089551092256571393  13379712   \n",
      "22501  1089551239837294594  13379712   \n",
      "22502  1089551307801800709  13379712   \n",
      "22503  1089551538832449537  13379712   \n",
      "\n",
      "                                                   clean  label  \n",
      "22499  rt user: cop: your neighbor has the word butth...      1  \n",
      "22500  rt user: [prison]  prisoner: what's for breakf...      1  \n",
      "22501  rt user: suspect: i ainâ€™t talkin  cop: [sharpe...      1  \n",
      "22502  rt user: [lighting a cigar] man i kicked my fa...      1  \n",
      "22503  rt user: me: do you wanna go skydiving  her: i...      1  \n",
      "Selecting 3200 posts from each user\n",
      "-----------15.560192346572876-----------\n",
      "0    1699\n",
      "1     496\n",
      "Name: label, dtype: int64\n",
      "Input number:  2195\n",
      "Label number:  2195\n",
      "Tokenizing text\n",
      "Total 808853 unique tokens.\n",
      "-----------220.50304102897644-----------\n",
      "Transforming text to input sequences\n",
      "-----------402.29203629493713-----------\n",
      "Finished loading data\n",
      "Shape of data tensor: (2195, 35000)\n",
      "Shape of label tensor: (2195, 2)\n",
      "Building embedding matrix\n",
      "50000 Selected word tokens\n",
      "-----------0.13763117790222168-----------\n",
      "--------------Start training----------------2020-02-20 00:50:12.214291\n",
      "Number of positive and negative classes in training and validation set\n",
      "[1359.  397.]\n",
      "[340.  99.]\n",
      "model fitting - Hierachical LSTM\n",
      "{0: 1.0, 1: 3.4231738035264483}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 35000)             0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 35000, 100)        5000000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 35000, 100)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 34998, 250)        75250     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 502       \n",
      "=================================================================\n",
      "Total params: 5,138,502\n",
      "Trainable params: 138,502\n",
      "Non-trainable params: 5,000,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\ipykernel_launcher.py:200: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Finished training----------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.random import seed\n",
    "from utils import split_data\n",
    "from utils.tokeniser import Tokenizer\n",
    "from utils.utils import get_class_weights\n",
    "from utils.embed import load_glove_tw_vectors\n",
    "from utils.users import load_users, select_post, combine_users\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from keras.optimizers import Adam, AdaMod\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers import Embedding, Dropout, Input\n",
    "from keras.models import Model\n",
    "from tensorflow import set_random_seed\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def create_dl_model():\n",
    "    num_filters = 250\n",
    "    drop = 0.5\n",
    "\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                input_length=INPUT_LENGTH,\n",
    "                                weights=[embedding_matrix],\n",
    "                                trainable=False)\n",
    "                                # trainable=True)\n",
    "\n",
    "    model_input = Input(shape=(INPUT_LENGTH,), dtype='int32')\n",
    "    model_layer = embedding_layer(model_input)\n",
    "    model_layer = Dropout(drop)(model_layer)\n",
    "    model_layer = Conv1D(filters=num_filters, kernel_size=3, activation='relu')(model_layer)\n",
    "    model_layer = GlobalMaxPooling1D()(model_layer)\n",
    "    model_layer = Dense(num_filters, activation='relu')(model_layer)\n",
    "    model_layer = Dropout(drop)(model_layer)\n",
    "    model_layer = Dense(2, activation='softmax')(model_layer)\n",
    "    # model_layer = Dense(1, activation='sigmoid')(model_layer)\n",
    "    model = Model(model_input, model_layer)\n",
    "\n",
    "    print(model.summary())\n",
    "#     adam = Adam(lr=0.007, clipvalue=7)\n",
    "    adam = AdaMod()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])  # metrics.binary_accuracy\n",
    "    return model\n",
    "\n",
    "\n",
    "SEED = 1\n",
    "seed(SEED)\n",
    "set_random_seed(SEED)\n",
    "\n",
    "POST_SIZE = 100\n",
    "MAX_POSTS = 3200\n",
    "MAX_NB_WORDS = 50000\n",
    "INPUT_LENGTH = 35000\n",
    "EMBEDDING_DIM = 100\n",
    "NB_DEPRESS = 1984\n",
    "\n",
    "DIR = '../data/'\n",
    "\n",
    "inputs, labels = load_users(DIR, POST_SIZE, MAX_POSTS, NB_DEPRESS, True)\n",
    "control = inputs[inputs.label == 0].reset_index(drop=True).copy()\n",
    "depress = inputs[inputs.label == 1].reset_index(drop=True).copy()\n",
    "del inputs, labels\n",
    "\n",
    "embeddings_index = load_glove_tw_vectors(EMBEDDING_DIM)  # load pre-trained embedding vectors\n",
    "\n",
    "i = 0\n",
    "MODEL_FOLDER = \"logs/{}/{}\".format('DL', datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M\") + '-' + str(MAX_NB_WORDS) + '-' + str(INPUT_LENGTH))\n",
    "for data_index in split_data.split(depress, 4):\n",
    "    data_fold = depress[depress.userid.isin(data_index)].copy()\n",
    "\n",
    "    print(data_index[:10])\n",
    "    print(data_fold.head())\n",
    "\n",
    "    combine, labels = combine_users(control, data_fold)\n",
    "    inputs, labels = select_post(combine, labels, MAX_POSTS)\n",
    "    del combine, data_fold\n",
    "\n",
    "    # transform Y to categories\n",
    "    print(labels.label.value_counts())\n",
    "    labels = labels.label.values\n",
    "    labels = to_categorical(np.asarray(labels))\n",
    "\n",
    "    print('Input number: ', len(inputs))\n",
    "    print('Label number: ', len(labels))\n",
    "\n",
    "    # alltexts = np.hstack(np.array(inputs))\n",
    "    alltexts = np.hstack(np.array(inputs).flatten())\n",
    "\n",
    "    print('Tokenizing text')\n",
    "    start = time.time()\n",
    "    tknzr = TweetTokenizer(reduce_len=True)\n",
    "    alltexts = [tknzr.tokenize(text.lower()) for text in alltexts]\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(alltexts)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Total %s unique tokens.' % len(word_index))\n",
    "    print('-----------{}-----------'.format(time.time() - start))\n",
    "\n",
    "    print('Transforming text to input sequences')\n",
    "#     start = time.time()\n",
    "#     data = np.zeros((len(inputs), INPUT_LENGTH), dtype='int32')\n",
    "#     for i, posts in enumerate(inputs):\n",
    "#         sequences = tokenizer.texts_to_sequences([' '.join(posts)])\n",
    "#         seq_data = pad_sequences(sequences, maxlen=INPUT_LENGTH, padding='pre', truncating='post')\n",
    "#         data[i] = seq_data\n",
    "#     del inputs, alltexts\n",
    "#     print('-----------{}-----------'.format(time.time() - start))\n",
    "    \n",
    "    start = time.time()\n",
    "    data = np.zeros((len(inputs), INPUT_LENGTH), dtype='int32')\n",
    "    for i, posts in enumerate(inputs):\n",
    "        ppp = []\n",
    "        for j, post in enumerate(posts):\n",
    "            pp = tokenizer.texts_to_sequences([post])\n",
    "            if len(pp[0]) > 18:\n",
    "                ppp = ppp + pp[0][:18]\n",
    "            else:\n",
    "                ppp = ppp + pp[0]\n",
    "        sequences = [ppp]\n",
    "        seq_data = pad_sequences(sequences, maxlen=INPUT_LENGTH, padding='pre', truncating='post')\n",
    "        data[i] = seq_data\n",
    "    del inputs, alltexts\n",
    "    print('-----------{}-----------'.format(time.time() - start))\n",
    "\n",
    "    print('Finished loading data')\n",
    "    print('Shape of data tensor:', data.shape)\n",
    "    print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "    num_words = min(MAX_NB_WORDS, len(word_index) + 1)\n",
    "\n",
    "    print('Building embedding matrix')\n",
    "    print('%s Selected word tokens' % num_words)\n",
    "\n",
    "    start = time.time()\n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "    # embedding_matrix = np.random.uniform(-1, 1, (num_words, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= MAX_NB_WORDS:\n",
    "            break\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print('-----------{}-----------'.format(time.time() - start))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=12345)\n",
    "    for train_index, test_index in skf.split(np.asarray(data), labels[:, 1]):\n",
    "        np.random.seed(0)\n",
    "        tf.set_random_seed(0)\n",
    "        sess = tf.Session(graph=tf.get_default_graph())\n",
    "        K.set_session(sess)\n",
    "\n",
    "        print('--------------Start training----------------{}'.format(datetime.datetime.now()))\n",
    "        X_train, x_val = copy.deepcopy(data[train_index]), copy.deepcopy(data[test_index])\n",
    "        y_train, y_val = copy.deepcopy(labels[train_index]), copy.deepcopy(labels[test_index])\n",
    "#         del data, labels\n",
    "\n",
    "        print('Number of positive and negative classes in training and validation set')\n",
    "        print(y_train.sum(axis=0))\n",
    "        print(y_val.sum(axis=0))\n",
    "\n",
    "        class_weight = get_class_weights(np.asarray(y_train, 'int32')[:, 1])\n",
    "\n",
    "        FOLDER = \"{}/{}\".format(MODEL_FOLDER, datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M\"))\n",
    "        if not os.path.exists(FOLDER):\n",
    "            os.makedirs(FOLDER)\n",
    "        callbacks = TensorBoard(log_dir=FOLDER,\n",
    "                                write_graph=True, write_grads=False, histogram_freq=0,\n",
    "                                write_images=True, embeddings_freq=0, embeddings_layer_names='embedding_1',\n",
    "                                embeddings_metadata=None)\n",
    "\n",
    "        checkpoint = ModelCheckpoint(FOLDER + '/model.{epoch:02d}-{val_acc:.2f}.hdf5',\n",
    "                                     verbose=0, monitor='val_acc',\n",
    "                                     save_best_only=True, mode='auto')\n",
    "\n",
    "        print(\"model fitting - Hierachical LSTM\")\n",
    "\n",
    "        print(class_weight)\n",
    "        model = create_dl_model()\n",
    "\n",
    "        model.fit(X_train, np.asarray(y_train, 'int32'),\n",
    "        # model.fit(X_train, np.asarray(y_train[:, 1], 'int32'),\n",
    "                  validation_data=(x_val, np.asarray(y_val, 'int32')),\n",
    "                  # validation_data=(x_val, np.asarray(y_val[:, 1], 'int32')),\n",
    "                  shuffle=False,\n",
    "                  nb_epoch=200, batch_size=32, verbose=0, class_weight=class_weight, callbacks=[checkpoint, callbacks])\n",
    "\n",
    "        model.save(FOLDER + '/my_model.h5')\n",
    "        del model, X_train, y_train, x_val, y_val, class_weight\n",
    "        K.clear_session()\n",
    "        print('--------------Finished training----------------'.format(datetime.datetime.now()))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dl_model():\n",
    "    num_filters = 250\n",
    "    drop = 0.2\n",
    "\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                input_length=INPUT_LENGTH,\n",
    "                                weights=[embedding_matrix],\n",
    "                                trainable=False)\n",
    "                                # trainable=True)\n",
    "\n",
    "    model_input = Input(shape=(INPUT_LENGTH,), dtype='int32')\n",
    "    model_layer = embedding_layer(model_input)\n",
    "    model_layer = Dropout(drop)(model_layer)\n",
    "    model_layer = Conv1D(filters=num_filters, kernel_size=3)(model_layer)\n",
    "    model_layer = GlobalMaxPooling1D()(model_layer)\n",
    "#     model_layer = Dropout(drop)(model_layer)\n",
    "    model_layer = Dense(num_filters, activation='relu')(model_layer)\n",
    "    model_layer = Dropout(drop)(model_layer)\n",
    "    model_layer = Dense(2, activation='softmax')(model_layer)\n",
    "    # model_layer = Dense(1, activation='sigmoid')(model_layer)\n",
    "    model = Model(model_input, model_layer)\n",
    "\n",
    "    print(model.summary())\n",
    "#     adam = Adam(lr=0.007, clipnorm=7)\n",
    "    adam = AdaMod()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])  # metrics.binary_accuracy\n",
    "    return model\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "K.set_session(sess)\n",
    "\n",
    "print('--------------Start training----------------{}'.format(datetime.datetime.now()))\n",
    "# X_train, x_val = copy.deepcopy(data[train_index]), copy.deepcopy(data[test_index])\n",
    "# y_train, y_val = copy.deepcopy(labels[train_index]), copy.deepcopy(labels[test_index])\n",
    "# del data, labels\n",
    "\n",
    "print('Number of positive and negative classes in training and validation set')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_val.sum(axis=0))\n",
    "\n",
    "class_weight = get_class_weights(np.asarray(y_train, 'int32')[:, 1])\n",
    "\n",
    "FOLDER = \"{}/{}\".format(MODEL_FOLDER, datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M\"))\n",
    "if not os.path.exists(FOLDER):\n",
    "    os.makedirs(FOLDER)\n",
    "callbacks = TensorBoard(log_dir=FOLDER,\n",
    "                        write_graph=True, write_grads=False, histogram_freq=0,\n",
    "                        write_images=True, embeddings_freq=0, embeddings_layer_names='embedding_1',\n",
    "                        embeddings_metadata=None)\n",
    "\n",
    "checkpoint = ModelCheckpoint(FOLDER + '/model.{epoch:02d}-{val_acc:.2f}.hdf5',\n",
    "                             verbose=0, monitor='val_acc',\n",
    "                             save_best_only=True, mode='auto')\n",
    "checkpoint = ModelCheckpoint(FOLDER + '/model.{epoch:02d}-{val_acc:.2f}.hdf5')\n",
    "print(\"model fitting - Hierachical LSTM\")\n",
    "\n",
    "print(class_weight)\n",
    "model = create_dl_model()\n",
    "\n",
    "model.fit(X_train, np.asarray(y_train, 'int32'),\n",
    "# model.fit(X_train, np.asarray(y_train[:, 1], 'int32'),\n",
    "#           validation_data=(x_val, np.asarray(y_val, 'int32')),\n",
    "          # validation_data=(x_val, np.asarray(y_val[:, 1], 'int32')),\n",
    "          shuffle=True,\n",
    "          nb_epoch=200, batch_size=32, verbose=0, class_weight=class_weight, callbacks=[ callbacks])\n",
    "\n",
    "model.save(FOLDER + '/my_model.h5')\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from utils.metrics import find_optimal_cutoff, get_max_acc\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'logs/DL/2020-02-19 15-04-50000-35000/2020-02-19 16-29/my_model.h5'\n",
    "\n",
    "print(model_name)\n",
    "model = load_model(model_name,\n",
    "                   custom_objects={'optimizer': AdaMod})\n",
    "probas_ = model.predict(x_val)\n",
    "y_test = y_val[:, 1].astype(int)\n",
    "max_accuracy, thresh = get_max_acc(y_test, probas_[:, 1])\n",
    "pred = [1 if m > thresh else 0 for m in probas_[:, 1]]\n",
    "print('Fold {} accuracy = {:.2f}'.format(0+1, max_accuracy*100))\n",
    "print(classification_report(y_test, pred, target_names=['Control', 'Depress']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "MODEL_FOLDER = \"logs/{}/{}\".format('DL', datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M\") + '-' + str(MAX_NB_WORDS) + '-' + str(INPUT_LENGTH))\n",
    "for data_index in split_data.split(depress, 4):\n",
    "    data_fold = depress[depress.userid.isin(data_index)].copy()\n",
    "\n",
    "    print(data_index[:10])\n",
    "    print(data_fold.head())\n",
    "\n",
    "    combine, labels = combine_users(control, data_fold)\n",
    "    inputs, labels = select_post(combine, labels, MAX_POSTS)\n",
    "    del combine, data_fold\n",
    "\n",
    "    # transform Y to categories\n",
    "    print(labels.label.value_counts())\n",
    "    labels = labels.label.values\n",
    "    labels = to_categorical(np.asarray(labels))\n",
    "\n",
    "    print('Input number: ', len(inputs))\n",
    "    print('Label number: ', len(labels))\n",
    "\n",
    "    # alltexts = np.hstack(np.array(inputs))\n",
    "    alltexts = np.hstack(np.array(inputs).flatten())\n",
    "\n",
    "    print('Tokenizing text')\n",
    "    start = time.time()\n",
    "    tknzr = TweetTokenizer(reduce_len=True)\n",
    "    alltexts = [tknzr.tokenize(text.lower()) for text in alltexts]\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(alltexts)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Total %s unique tokens.' % len(word_index))\n",
    "    print('-----------{}-----------'.format(time.time() - start))\n",
    "\n",
    "    print('Transforming text to input sequences')\n",
    "    start = time.time()\n",
    "    data = np.zeros((len(inputs), INPUT_LENGTH), dtype='int32')\n",
    "    ppp = []\n",
    "    for i, posts in enumerate(inputs):\n",
    "        for j, post in enumerate(posts):\n",
    "            pp = tokenizer.texts_to_sequences([post])\n",
    "            if len(pp) > 18:\n",
    "                ppp = ppp + pp[:18]\n",
    "            else:\n",
    "                ppp = ppp + pp\n",
    "        sequences = tokenizer.texts_to_sequences([' '.join(posts)])\n",
    "        seq_data = pad_sequences(sequences, maxlen=INPUT_LENGTH)\n",
    "        data[i] = seq_data\n",
    "#     del inputs, alltexts\n",
    "    print('-----------{}-----------'.format(time.time() - start))\n",
    "    \n",
    "    for i, posts in enumerate(inputs):\n",
    "        for j, post in enumerate(posts):\n",
    "            if j < MAX_POSTS:\n",
    "                sequences = tokenizer.texts_to_sequences([post])\n",
    "                seq_data = pad_sequences(sequences, maxlen=MAX_POST_LENGTH)\n",
    "                data[i, j] = seq_data\n",
    "    elapsed_time_fl = (time.time() - start)\n",
    "    print(elapsed_time_fl)\n",
    "\n",
    "    print('Finished loading data')\n",
    "    print('Shape of data tensor:', data.shape)\n",
    "    print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "    num_words = min(MAX_NB_WORDS, len(word_index) + 1)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=12345)\n",
    "    for train_index, test_index in skf.split(np.asarray(data), labels[:, 1]):\n",
    "        np.random.seed(0)\n",
    "        tf.set_random_seed(0)\n",
    "        sess = tf.Session(graph=tf.get_default_graph())\n",
    "        K.set_session(sess)\n",
    "\n",
    "        print('--------------Start training----------------{}'.format(datetime.datetime.now()))\n",
    "        X_train, x_val = copy.deepcopy(data[train_index]), copy.deepcopy(data[test_index])\n",
    "        y_train, y_val = copy.deepcopy(labels[train_index]), copy.deepcopy(labels[test_index])\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FOLDER = \"logs/{}/{}\".format('DL', datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M\") + '-' + str(MAX_NB_WORDS) + '-' + str(INPUT_LENGTH))\n",
    "for data_index in split_data.split(depress, 4):\n",
    "    data_fold = depress[depress.userid.isin(data_index)].copy()\n",
    "\n",
    "    print(data_index[:10])\n",
    "    print(data_fold.head())\n",
    "\n",
    "    combine, labels = combine_users(control, data_fold)\n",
    "    inputs, labels = select_post(combine, labels, MAX_POSTS)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.zeros((len(inputs), INPUT_LENGTH), dtype='int32')\n",
    "for i, posts in enumerate(inputs[:2]):\n",
    "    ppp = []\n",
    "    for j, post in enumerate(posts):\n",
    "        pp = tokenizer.texts_to_sequences([post])\n",
    "        if len(pp[0]) > 18:\n",
    "            ppp = ppp + pp[0][-18:]\n",
    "        else:\n",
    "            ppp = ppp + pp[0]\n",
    "    sequences = [ppp]\n",
    "    seq_data = pad_sequences(sequences, maxlen=INPUT_LENGTH)\n",
    "    data[i] = seq_data\n",
    "#     del inputs, alltexts\n",
    "print('-----------{}-----------'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = np.zeros((len(inputs), INPUT_LENGTH), dtype='int32')\n",
    "for i, posts in enumerate(inputs):\n",
    "    sequences = tokenizer.texts_to_sequences([' '.join(posts)])\n",
    "    seq_data = pad_sequences(sequences, maxlen=INPUT_LENGTH)\n",
    "    data2[i] = seq_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of control:  1699\n",
      "The number of depress:  1984\n",
      "Control shape:  (3944945, 3)\n",
      "Depress shape:  (4996621, 3)\n",
      "Selecting users with at least 100 posts\n",
      "Control users:  1699\n",
      "-----------12.117253303527832-----------\n",
      "Depress users:  1984\n",
      "-----------16.086098670959473-----------\n",
      "Labelling\n",
      "Loading embedding vectors from a file\n",
      "Total 1193514 word vectors.\n",
      "[ 780078428687065089            25567061           416684112\n",
      "           608725707  912541969968066560          2700680921\n",
      "          2329059770 1064934140007538688            43903078\n",
      "            16656127]\n",
      "                  tweetid   userid  \\\n",
      "3234  1143666788107866112  5882452   \n",
      "3235  1143666924246560768  5882452   \n",
      "3236  1143664692679720961  5882452   \n",
      "3237  1143661172891066370  5882452   \n",
      "3238  1143669737378893824  5882452   \n",
      "\n",
      "                                                  clean  label  \n",
      "3234  user user i've always felt like i was kind of ...      1  \n",
      "3235  user user i cannot function without my morning...      1  \n",
      "3236  user way too much of what you're describing so...      1  \n",
      "3237  user between chronic pain, anxiety, and depres...      1  \n",
      "3238  user user benadryl stops my itching but does n...      1  \n",
      "Selecting 3200 posts from each user\n",
      "-----------15.738409996032715-----------\n",
      "0    1699\n",
      "1     496\n",
      "Name: label, dtype: int64\n",
      "Input number:  2195\n",
      "Label number:  2195\n",
      "Tokenizing text\n",
      "Total 807701 unique tokens.\n",
      "-----------213.35295748710632-----------\n",
      "Transforming text to input sequences\n",
      "-----------407.0389664173126-----------\n",
      "Finished loading data\n",
      "Shape of data tensor: (2195, 35000)\n",
      "Shape of label tensor: (2195, 2)\n",
      "Building embedding matrix\n",
      "50000 Selected word tokens\n",
      "-----------0.08682870864868164-----------\n",
      "--------------Start training----------------2020-02-20 15:45:51.290133\n",
      "Number of positive and negative classes in training and validation set\n",
      "[1359.  397.]\n",
      "[340.  99.]\n",
      "model fitting - Hierachical LSTM\n",
      "{0: 1.0, 1: 3.4231738035264483}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 35000)             0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 35000, 100)        5000000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 35000, 100)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 34998, 250)        75250     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 502       \n",
      "=================================================================\n",
      "Total params: 5,138,502\n",
      "Trainable params: 138,502\n",
      "Non-trainable params: 5,000,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\ipykernel_launcher.py:202: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Finished training----------------\n",
      "[         2826065615          3960721393          4318344515\n",
      "          3382208403          2599342730          2192916252\n",
      "           181892265 1091181448843931649  836044275712475136\n",
      "            60688549]\n",
      "                  tweetid   userid  \\\n",
      "6479  1117414019164454912  6111852   \n",
      "6480  1117414158188929024  6111852   \n",
      "6481  1117413476631220226  6111852   \n",
      "6482  1116951765906227200  6111852   \n",
      "6483  1117037694268342272  6111852   \n",
      "\n",
      "                                                  clean  label  \n",
      "6479                     good morning, twitterverse url      1  \n",
      "6480  rt user: everything you need to know about k-p...      1  \n",
      "6481  it's also a rhetorical trick thwimp uses all t...      1  \n",
      "6482                          rt user: this thread: url      1  \n",
      "6483  putin has said america is no longer a leader. ...      1  \n",
      "Selecting 3200 posts from each user\n",
      "-----------18.413405656814575-----------\n",
      "0    1699\n",
      "1     496\n",
      "Name: label, dtype: int64\n",
      "Input number:  2195\n",
      "Label number:  2195\n",
      "Tokenizing text\n",
      "Total 804337 unique tokens.\n",
      "-----------238.1429934501648-----------\n",
      "Transforming text to input sequences\n",
      "-----------412.80095624923706-----------\n",
      "Finished loading data\n",
      "Shape of data tensor: (2195, 35000)\n",
      "Shape of label tensor: (2195, 2)\n",
      "Building embedding matrix\n",
      "50000 Selected word tokens\n",
      "-----------0.09873390197753906-----------\n",
      "--------------Start training----------------2020-02-20 16:02:37.689427\n",
      "Number of positive and negative classes in training and validation set\n",
      "[1359.  397.]\n",
      "[340.  99.]\n",
      "model fitting - Hierachical LSTM\n",
      "{0: 1.0, 1: 3.4231738035264483}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 35000)             0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 35000, 100)        5000000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 35000, 100)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 34998, 250)        75250     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 502       \n",
      "=================================================================\n",
      "Total params: 5,138,502\n",
      "Trainable params: 138,502\n",
      "Non-trainable params: 5,000,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\ipykernel_launcher.py:202: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Finished training----------------\n",
      "[           22936389          1586758399          3227968831\n",
      "          2731028615          3056177975            25787341\n",
      " 1104780414516977666           280932608            56891919\n",
      "  800940814641856512]\n",
      "               tweetid  userid  \\\n",
      "0   589962698353299457  802310   \n",
      "1  1130666193289863168  802310   \n",
      "2  1070479497898520577  802310   \n",
      "3  1070482628153794560  802310   \n",
      "4  1070483237879799809  802310   \n",
      "\n",
      "                                               clean  label  \n",
      "0  user github is giving me more grief than the l...      1  \n",
      "1  user some of us just ride the risk-train and g...      1  \n",
      "2  user super happy to see another competent deve...      1  \n",
      "3  user you can't trust lnp to make sensible cybe...      1  \n",
      "4  user user is user prepared to stand against <h...      1  \n",
      "Selecting 3200 posts from each user\n",
      "-----------18.52982783317566-----------\n",
      "0    1699\n",
      "1     496\n",
      "Name: label, dtype: int64\n",
      "Input number:  2195\n",
      "Label number:  2195\n",
      "Tokenizing text\n",
      "Total 793449 unique tokens.\n",
      "-----------239.67132019996643-----------\n",
      "Transforming text to input sequences\n",
      "-----------422.59065198898315-----------\n",
      "Finished loading data\n",
      "Shape of data tensor: (2195, 35000)\n",
      "Shape of label tensor: (2195, 2)\n",
      "Building embedding matrix\n",
      "50000 Selected word tokens\n",
      "-----------0.08876466751098633-----------\n",
      "--------------Start training----------------2020-02-20 16:20:39.701229\n",
      "Number of positive and negative classes in training and validation set\n",
      "[1359.  397.]\n",
      "[340.  99.]\n",
      "model fitting - Hierachical LSTM\n",
      "{0: 1.0, 1: 3.4231738035264483}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 35000)             0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 35000, 100)        5000000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 35000, 100)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 34998, 250)        75250     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 502       \n",
      "=================================================================\n",
      "Total params: 5,138,502\n",
      "Trainable params: 138,502\n",
      "Non-trainable params: 5,000,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\ipykernel_launcher.py:202: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Finished training----------------\n",
      "[ 960112188433338368 1004044855931293696           497519459\n",
      "            41555588  753773198383128576          2868511826\n",
      " 1019322256953106437  962874724987551744 1004689374221623297\n",
      "          2237542326]\n",
      "                   tweetid    userid  \\\n",
      "22499  1089551448323575809  13379712   \n",
      "22500  1089551092256571393  13379712   \n",
      "22501  1089551239837294594  13379712   \n",
      "22502  1089551307801800709  13379712   \n",
      "22503  1089551538832449537  13379712   \n",
      "\n",
      "                                                   clean  label  \n",
      "22499  rt user: cop: your neighbor has the word butth...      1  \n",
      "22500  rt user: [prison]  prisoner: what's for breakf...      1  \n",
      "22501  rt user: suspect: i ainâ€™t talkin  cop: [sharpe...      1  \n",
      "22502  rt user: [lighting a cigar] man i kicked my fa...      1  \n",
      "22503  rt user: me: do you wanna go skydiving  her: i...      1  \n",
      "Selecting 3200 posts from each user\n",
      "-----------15.451961278915405-----------\n",
      "0    1699\n",
      "1     496\n",
      "Name: label, dtype: int64\n",
      "Input number:  2195\n",
      "Label number:  2195\n",
      "Tokenizing text\n",
      "Total 808853 unique tokens.\n",
      "-----------214.70110273361206-----------\n",
      "Transforming text to input sequences\n",
      "-----------391.08444356918335-----------\n",
      "Finished loading data\n",
      "Shape of data tensor: (2195, 35000)\n",
      "Shape of label tensor: (2195, 2)\n",
      "Building embedding matrix\n",
      "50000 Selected word tokens\n",
      "-----------0.08774876594543457-----------\n",
      "--------------Start training----------------2020-02-20 16:37:10.132539\n",
      "Number of positive and negative classes in training and validation set\n",
      "[1359.  397.]\n",
      "[340.  99.]\n",
      "model fitting - Hierachical LSTM\n",
      "{0: 1.0, 1: 3.4231738035264483}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 35000)             0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 35000, 100)        5000000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 35000, 100)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 34998, 250)        75250     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 502       \n",
      "=================================================================\n",
      "Total params: 5,138,502\n",
      "Trainable params: 138,502\n",
      "Non-trainable params: 5,000,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\ipykernel_launcher.py:202: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Finished training----------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.random import seed\n",
    "from utils import split_data\n",
    "from utils.tokeniser import Tokenizer\n",
    "from utils.utils import get_class_weights\n",
    "from utils.embed import load_glove_tw_vectors\n",
    "from utils.users import load_users, select_post, combine_users\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from keras.optimizers import Adam, AdaMod\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers import Embedding, Dropout, Input\n",
    "from keras.models import Model\n",
    "from tensorflow import set_random_seed\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def create_dl_model():\n",
    "    num_filters = 250\n",
    "    drop = 0.5\n",
    "\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                input_length=INPUT_LENGTH,\n",
    "                                weights=[embedding_matrix],\n",
    "                                trainable=False)\n",
    "                                # trainable=True)\n",
    "\n",
    "    model_input = Input(shape=(INPUT_LENGTH,), dtype='int32')\n",
    "    model_layer = embedding_layer(model_input)\n",
    "    model_layer = Dropout(drop)(model_layer)\n",
    "    model_layer = Conv1D(filters=num_filters, kernel_size=3)(model_layer)\n",
    "    model_layer = GlobalMaxPooling1D()(model_layer)\n",
    "    model_layer = Dense(num_filters, activation='relu')(model_layer)\n",
    "    model_layer = Dropout(drop)(model_layer)\n",
    "    model_layer = Dense(2, activation='softmax')(model_layer)\n",
    "    # model_layer = Dense(1, activation='sigmoid')(model_layer)\n",
    "    model = Model(model_input, model_layer)\n",
    "\n",
    "    print(model.summary())\n",
    "#     adam = Adam(lr=0.007, clipvalue=7)\n",
    "    adam = AdaMod()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])  # metrics.binary_accuracy\n",
    "    return model\n",
    "\n",
    "\n",
    "SEED = 1\n",
    "seed(SEED)\n",
    "set_random_seed(SEED)\n",
    "\n",
    "POST_SIZE = 100\n",
    "MAX_POSTS = 3200\n",
    "MAX_NB_WORDS = 50000\n",
    "INPUT_LENGTH = 35000\n",
    "EMBEDDING_DIM = 100\n",
    "NB_DEPRESS = 1984\n",
    "\n",
    "DIR = '../data/'\n",
    "\n",
    "inputs, labels = load_users(DIR, POST_SIZE, MAX_POSTS, NB_DEPRESS, True)\n",
    "control = inputs[inputs.label == 0].reset_index(drop=True).copy()\n",
    "depress = inputs[inputs.label == 1].reset_index(drop=True).copy()\n",
    "del inputs, labels\n",
    "\n",
    "embeddings_index = load_glove_tw_vectors(EMBEDDING_DIM)  # load pre-trained embedding vectors\n",
    "\n",
    "i = 0\n",
    "MODEL_FOLDER = \"logs/{}/{}\".format('DL', datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M\") + '-' + str(MAX_NB_WORDS) + '-' + str(INPUT_LENGTH))\n",
    "for data_index in split_data.split(depress, 4):\n",
    "    data_fold = depress[depress.userid.isin(data_index)].copy()\n",
    "\n",
    "    print(data_index[:10])\n",
    "    print(data_fold.head())\n",
    "\n",
    "    combine, labels = combine_users(control, data_fold)\n",
    "    inputs, labels = select_post(combine, labels, MAX_POSTS)\n",
    "    del combine, data_fold\n",
    "\n",
    "    # transform Y to categories\n",
    "    print(labels.label.value_counts())\n",
    "    labels = labels.label.values\n",
    "    labels = to_categorical(np.asarray(labels))\n",
    "\n",
    "    print('Input number: ', len(inputs))\n",
    "    print('Label number: ', len(labels))\n",
    "\n",
    "    # alltexts = np.hstack(np.array(inputs))\n",
    "    alltexts = np.hstack(np.array(inputs).flatten())\n",
    "\n",
    "    print('Tokenizing text')\n",
    "    start = time.time()\n",
    "    tknzr = TweetTokenizer(reduce_len=True)\n",
    "    alltexts = [tknzr.tokenize(text.lower()) for text in alltexts]\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(alltexts)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Total %s unique tokens.' % len(word_index))\n",
    "    print('-----------{}-----------'.format(time.time() - start))\n",
    "\n",
    "    print('Transforming text to input sequences')\n",
    "#     start = time.time()\n",
    "#     data = np.zeros((len(inputs), INPUT_LENGTH), dtype='int32')\n",
    "#     for i, posts in enumerate(inputs):\n",
    "#         sequences = tokenizer.texts_to_sequences([' '.join(posts)])\n",
    "#         seq_data = pad_sequences(sequences, maxlen=INPUT_LENGTH, padding='pre', truncating='post')\n",
    "#         data[i] = seq_data\n",
    "#     del inputs, alltexts\n",
    "#     print('-----------{}-----------'.format(time.time() - start))\n",
    "    \n",
    "    start = time.time()\n",
    "    data = np.zeros((len(inputs), INPUT_LENGTH), dtype='int32')\n",
    "    for i, posts in enumerate(inputs):\n",
    "        ppp = []\n",
    "        for j, post in enumerate(posts):\n",
    "            pp = tokenizer.texts_to_sequences([post])\n",
    "            if len(pp[0]) > 18:\n",
    "                ppp = ppp + pp[0][:18]\n",
    "            else:\n",
    "                ppp = ppp + pp[0]\n",
    "        sequences = [ppp]\n",
    "        seq_data = pad_sequences(sequences, maxlen=INPUT_LENGTH, padding='pre', truncating='post')\n",
    "        data[i] = seq_data\n",
    "    del inputs, alltexts\n",
    "    print('-----------{}-----------'.format(time.time() - start))\n",
    "\n",
    "    print('Finished loading data')\n",
    "    print('Shape of data tensor:', data.shape)\n",
    "    print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "    num_words = min(MAX_NB_WORDS, len(word_index) + 1)\n",
    "\n",
    "    print('Building embedding matrix')\n",
    "    print('%s Selected word tokens' % num_words)\n",
    "\n",
    "    start = time.time()\n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "    # embedding_matrix = np.random.uniform(-1, 1, (num_words, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= MAX_NB_WORDS:\n",
    "            break\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print('-----------{}-----------'.format(time.time() - start))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=12345)\n",
    "    for train_index, test_index in skf.split(np.asarray(data), labels[:, 1]):\n",
    "        np.random.seed(0)\n",
    "        tf.set_random_seed(0)\n",
    "        sess = tf.Session(graph=tf.get_default_graph())\n",
    "        K.set_session(sess)\n",
    "\n",
    "        print('--------------Start training----------------{}'.format(datetime.datetime.now()))\n",
    "        X_train, x_val = copy.deepcopy(data[train_index]), copy.deepcopy(data[test_index])\n",
    "        y_train, y_val = copy.deepcopy(labels[train_index]), copy.deepcopy(labels[test_index])\n",
    "#         del data, labels\n",
    "\n",
    "        print('Number of positive and negative classes in training and validation set')\n",
    "        print(y_train.sum(axis=0))\n",
    "        print(y_val.sum(axis=0))\n",
    "\n",
    "        class_weight = get_class_weights(np.asarray(y_train, 'int32')[:, 1])\n",
    "\n",
    "        FOLDER = \"{}/{}\".format(MODEL_FOLDER, datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M\"))\n",
    "        if not os.path.exists(FOLDER):\n",
    "            os.makedirs(FOLDER)\n",
    "        callbacks = TensorBoard(log_dir=FOLDER,\n",
    "                                write_graph=True, write_grads=False, histogram_freq=0,\n",
    "                                write_images=True, embeddings_freq=0, embeddings_layer_names='embedding_1',\n",
    "                                embeddings_metadata=None)\n",
    "\n",
    "        checkpoint = ModelCheckpoint(FOLDER + '/model.{epoch:02d}-{val_acc:.2f}.hdf5',\n",
    "                                     verbose=0, monitor='val_acc',\n",
    "                                     save_best_only=True, mode='auto')\n",
    "        \n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=15)\n",
    "\n",
    "        print(\"model fitting - Hierachical LSTM\")\n",
    "\n",
    "        print(class_weight)\n",
    "        model = create_dl_model()\n",
    "\n",
    "        model.fit(X_train, np.asarray(y_train, 'int32'),\n",
    "        # model.fit(X_train, np.asarray(y_train[:, 1], 'int32'),\n",
    "                  validation_data=(x_val, np.asarray(y_val, 'int32')),\n",
    "                  # validation_data=(x_val, np.asarray(y_val[:, 1], 'int32')),\n",
    "                  shuffle=False,\n",
    "                  nb_epoch=200, batch_size=32, verbose=0, class_weight=class_weight, callbacks=[checkpoint, callbacks, es])\n",
    "\n",
    "        model.save(FOLDER + '/my_model.h5')\n",
    "        del model, X_train, y_train, x_val, y_val, class_weight\n",
    "        K.clear_session()\n",
    "        print('--------------Finished training----------------'.format(datetime.datetime.now()))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of control:  1699\n",
      "The number of depress:  1984\n",
      "Control shape:  (3944945, 3)\n",
      "Depress shape:  (4996621, 3)\n",
      "Selecting users with at least 100 posts\n",
      "Control users:  1699\n",
      "-----------12.201915979385376-----------\n",
      "Depress users:  1984\n",
      "-----------16.185059785842896-----------\n",
      "Labelling\n",
      "Loading embedding vectors from a file\n",
      "Total 1193514 word vectors.\n",
      "[ 780078428687065089            25567061           416684112\n",
      "           608725707  912541969968066560          2700680921\n",
      "          2329059770 1064934140007538688            43903078\n",
      "            16656127]\n",
      "                  tweetid   userid  \\\n",
      "3234  1143666788107866112  5882452   \n",
      "3235  1143666924246560768  5882452   \n",
      "3236  1143664692679720961  5882452   \n",
      "3237  1143661172891066370  5882452   \n",
      "3238  1143669737378893824  5882452   \n",
      "\n",
      "                                                  clean  label  \n",
      "3234  user user i've always felt like i was kind of ...      1  \n",
      "3235  user user i cannot function without my morning...      1  \n",
      "3236  user way too much of what you're describing so...      1  \n",
      "3237  user between chronic pain, anxiety, and depres...      1  \n",
      "3238  user user benadryl stops my itching but does n...      1  \n",
      "Selecting 3200 posts from each user\n",
      "-----------16.44349503517151-----------\n",
      "0    1699\n",
      "1     496\n",
      "Name: label, dtype: int64\n",
      "Input number:  2195\n",
      "Label number:  2195\n",
      "Tokenizing text\n",
      "Total 807701 unique tokens.\n",
      "-----------223.04823303222656-----------\n",
      "Transforming text to input sequences\n",
      "-----------202.05458116531372-----------\n",
      "Finished loading data\n",
      "Shape of data tensor: (2195, 35000)\n",
      "Shape of label tensor: (2195, 2)\n",
      "Building embedding matrix\n",
      "50000 Selected word tokens\n",
      "-----------0.09841489791870117-----------\n",
      "--------------Start training----------------2020-02-20 20:53:50.137622\n",
      "Number of positive and negative classes in training and validation set\n",
      "[1359.  397.]\n",
      "[340.  99.]\n",
      "model fitting - Hierachical LSTM\n",
      "{0: 1.0, 1: 3.4231738035264483}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 35000)             0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 35000, 100)        5000000   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 35000, 100)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 34998, 250)        75250     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 502       \n",
      "=================================================================\n",
      "Total params: 5,138,502\n",
      "Trainable params: 138,502\n",
      "Non-trainable params: 5,000,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\ipykernel_launcher.py:202: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Finished training----------------\n",
      "[         2826065615          3960721393          4318344515\n",
      "          3382208403          2599342730          2192916252\n",
      "           181892265 1091181448843931649  836044275712475136\n",
      "            60688549]\n",
      "                  tweetid   userid  \\\n",
      "6479  1117414019164454912  6111852   \n",
      "6480  1117414158188929024  6111852   \n",
      "6481  1117413476631220226  6111852   \n",
      "6482  1116951765906227200  6111852   \n",
      "6483  1117037694268342272  6111852   \n",
      "\n",
      "                                                  clean  label  \n",
      "6479                     good morning, twitterverse url      1  \n",
      "6480  rt user: everything you need to know about k-p...      1  \n",
      "6481  it's also a rhetorical trick thwimp uses all t...      1  \n",
      "6482                          rt user: this thread: url      1  \n",
      "6483  putin has said america is no longer a leader. ...      1  \n",
      "Selecting 3200 posts from each user\n",
      "-----------16.286903381347656-----------\n",
      "0    1699\n",
      "1     496\n",
      "Name: label, dtype: int64\n",
      "Input number:  2195\n",
      "Label number:  2195\n",
      "Tokenizing text\n",
      "Total 804337 unique tokens.\n",
      "-----------221.2274842262268-----------\n",
      "Transforming text to input sequences\n",
      "-----------203.16309261322021-----------\n",
      "Finished loading data\n",
      "Shape of data tensor: (2195, 35000)\n",
      "Shape of label tensor: (2195, 2)\n",
      "Building embedding matrix\n",
      "50000 Selected word tokens\n",
      "-----------0.0867002010345459-----------\n",
      "--------------Start training----------------2020-02-20 21:06:07.944464\n",
      "Number of positive and negative classes in training and validation set\n",
      "[1359.  397.]\n",
      "[340.  99.]\n",
      "model fitting - Hierachical LSTM\n",
      "{0: 1.0, 1: 3.4231738035264483}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 35000)             0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 35000, 100)        5000000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 35000, 100)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 34998, 250)        75250     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 502       \n",
      "=================================================================\n",
      "Total params: 5,138,502\n",
      "Trainable params: 138,502\n",
      "Non-trainable params: 5,000,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\ipykernel_launcher.py:202: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Finished training----------------\n",
      "[           22936389          1586758399          3227968831\n",
      "          2731028615          3056177975            25787341\n",
      " 1104780414516977666           280932608            56891919\n",
      "  800940814641856512]\n",
      "               tweetid  userid  \\\n",
      "0   589962698353299457  802310   \n",
      "1  1130666193289863168  802310   \n",
      "2  1070479497898520577  802310   \n",
      "3  1070482628153794560  802310   \n",
      "4  1070483237879799809  802310   \n",
      "\n",
      "                                               clean  label  \n",
      "0  user github is giving me more grief than the l...      1  \n",
      "1  user some of us just ride the risk-train and g...      1  \n",
      "2  user super happy to see another competent deve...      1  \n",
      "3  user you can't trust lnp to make sensible cybe...      1  \n",
      "4  user user is user prepared to stand against <h...      1  \n",
      "Selecting 3200 posts from each user\n",
      "-----------15.779103994369507-----------\n",
      "0    1699\n",
      "1     496\n",
      "Name: label, dtype: int64\n",
      "Input number:  2195\n",
      "Label number:  2195\n",
      "Tokenizing text\n",
      "Total 793449 unique tokens.\n",
      "-----------216.63447880744934-----------\n",
      "Transforming text to input sequences\n",
      "-----------200.48515701293945-----------\n",
      "Finished loading data\n",
      "Shape of data tensor: (2195, 35000)\n",
      "Shape of label tensor: (2195, 2)\n",
      "Building embedding matrix\n",
      "50000 Selected word tokens\n",
      "-----------0.0861661434173584-----------\n",
      "--------------Start training----------------2020-02-20 21:19:35.915256\n",
      "Number of positive and negative classes in training and validation set\n",
      "[1359.  397.]\n",
      "[340.  99.]\n",
      "model fitting - Hierachical LSTM\n",
      "{0: 1.0, 1: 3.4231738035264483}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 35000)             0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 35000, 100)        5000000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 35000, 100)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 34998, 250)        75250     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 502       \n",
      "=================================================================\n",
      "Total params: 5,138,502\n",
      "Trainable params: 138,502\n",
      "Non-trainable params: 5,000,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\ipykernel_launcher.py:202: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Finished training----------------\n",
      "[ 960112188433338368 1004044855931293696           497519459\n",
      "            41555588  753773198383128576          2868511826\n",
      " 1019322256953106437  962874724987551744 1004689374221623297\n",
      "          2237542326]\n",
      "                   tweetid    userid  \\\n",
      "22499  1089551448323575809  13379712   \n",
      "22500  1089551092256571393  13379712   \n",
      "22501  1089551239837294594  13379712   \n",
      "22502  1089551307801800709  13379712   \n",
      "22503  1089551538832449537  13379712   \n",
      "\n",
      "                                                   clean  label  \n",
      "22499  rt user: cop: your neighbor has the word butth...      1  \n",
      "22500  rt user: [prison]  prisoner: what's for breakf...      1  \n",
      "22501  rt user: suspect: i ainâ€™t talkin  cop: [sharpe...      1  \n",
      "22502  rt user: [lighting a cigar] man i kicked my fa...      1  \n",
      "22503  rt user: me: do you wanna go skydiving  her: i...      1  \n",
      "Selecting 3200 posts from each user\n",
      "-----------15.695070266723633-----------\n",
      "0    1699\n",
      "1     496\n",
      "Name: label, dtype: int64\n",
      "Input number:  2195\n",
      "Label number:  2195\n",
      "Tokenizing text\n",
      "Total 808853 unique tokens.\n",
      "-----------218.06882619857788-----------\n",
      "Transforming text to input sequences\n",
      "-----------199.06341290473938-----------\n",
      "Finished loading data\n",
      "Shape of data tensor: (2195, 35000)\n",
      "Shape of label tensor: (2195, 2)\n",
      "Building embedding matrix\n",
      "50000 Selected word tokens\n",
      "-----------0.09148573875427246-----------\n",
      "--------------Start training----------------2020-02-20 21:33:23.966593\n",
      "Number of positive and negative classes in training and validation set\n",
      "[1359.  397.]\n",
      "[340.  99.]\n",
      "model fitting - Hierachical LSTM\n",
      "{0: 1.0, 1: 3.4231738035264483}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 35000)             0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 35000, 100)        5000000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 35000, 100)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 34998, 250)        75250     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 502       \n",
      "=================================================================\n",
      "Total params: 5,138,502\n",
      "Trainable params: 138,502\n",
      "Non-trainable params: 5,000,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\ipykernel_launcher.py:202: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Finished training----------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.random import seed\n",
    "from utils import split_data\n",
    "from utils.tokeniser import Tokenizer\n",
    "from utils.utils import get_class_weights\n",
    "from utils.embed import load_glove_tw_vectors\n",
    "from utils.users import load_users, select_post, combine_users\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from keras.optimizers import Adam, AdaMod\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers import Embedding, Dropout, Input\n",
    "from keras.models import Model\n",
    "from tensorflow import set_random_seed\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def create_dl_model():\n",
    "    num_filters = 250\n",
    "    drop = 0.5\n",
    "\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                input_length=INPUT_LENGTH,\n",
    "                                weights=[embedding_matrix],\n",
    "                                trainable=False)\n",
    "                                # trainable=True)\n",
    "\n",
    "    model_input = Input(shape=(INPUT_LENGTH,), dtype='int32')\n",
    "    model_layer = embedding_layer(model_input)\n",
    "    model_layer = Dropout(drop)(model_layer)\n",
    "    model_layer = Conv1D(filters=num_filters, kernel_size=3)(model_layer)\n",
    "    model_layer = GlobalMaxPooling1D()(model_layer)\n",
    "    model_layer = Dense(num_filters, activation='relu')(model_layer)\n",
    "    model_layer = Dropout(drop)(model_layer)\n",
    "    model_layer = Dense(2, activation='softmax')(model_layer)\n",
    "    # model_layer = Dense(1, activation='sigmoid')(model_layer)\n",
    "    model = Model(model_input, model_layer)\n",
    "\n",
    "    print(model.summary())\n",
    "#     adam = Adam(lr=0.007, clipvalue=7)\n",
    "    adam = AdaMod()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])  # metrics.binary_accuracy\n",
    "    return model\n",
    "\n",
    "\n",
    "SEED = 1\n",
    "seed(SEED)\n",
    "set_random_seed(SEED)\n",
    "\n",
    "POST_SIZE = 100\n",
    "MAX_POSTS = 3200\n",
    "MAX_NB_WORDS = 50000\n",
    "INPUT_LENGTH = 35000\n",
    "EMBEDDING_DIM = 100\n",
    "NB_DEPRESS = 1984\n",
    "\n",
    "DIR = '../data/'\n",
    "\n",
    "inputs, labels = load_users(DIR, POST_SIZE, MAX_POSTS, NB_DEPRESS, True)\n",
    "control = inputs[inputs.label == 0].reset_index(drop=True).copy()\n",
    "depress = inputs[inputs.label == 1].reset_index(drop=True).copy()\n",
    "del inputs, labels\n",
    "\n",
    "embeddings_index = load_glove_tw_vectors(EMBEDDING_DIM)  # load pre-trained embedding vectors\n",
    "\n",
    "fold = 0\n",
    "MODEL_FOLDER = \"logs/{}/{}\".format('DL', datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M\") + '-' + str(MAX_NB_WORDS) + '-' + str(INPUT_LENGTH))\n",
    "for data_index in split_data.split(depress, 4):\n",
    "    data_fold = depress[depress.userid.isin(data_index)].copy()\n",
    "\n",
    "    print(data_index[:10])\n",
    "    print(data_fold.head())\n",
    "\n",
    "    combine, labels = combine_users(control, data_fold)\n",
    "    inputs, labels = select_post(combine, labels, MAX_POSTS)\n",
    "    del combine, data_fold\n",
    "\n",
    "    # transform Y to categories\n",
    "    print(labels.label.value_counts())\n",
    "    labels = labels.label.values\n",
    "    labels = to_categorical(np.asarray(labels))\n",
    "\n",
    "    print('Input number: ', len(inputs))\n",
    "    print('Label number: ', len(labels))\n",
    "\n",
    "    # alltexts = np.hstack(np.array(inputs))\n",
    "    alltexts = np.hstack(np.array(inputs).flatten())\n",
    "\n",
    "    print('Tokenizing text')\n",
    "    start = time.time()\n",
    "    tknzr = TweetTokenizer(reduce_len=True)\n",
    "    alltexts = [tknzr.tokenize(text.lower()) for text in alltexts]\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(alltexts)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Total %s unique tokens.' % len(word_index))\n",
    "    print('-----------{}-----------'.format(time.time() - start))\n",
    "\n",
    "    print('Transforming text to input sequences')\n",
    "    start = time.time()\n",
    "    data = np.zeros((len(inputs), INPUT_LENGTH), dtype='int32')\n",
    "    for i, posts in enumerate(inputs):\n",
    "        sequences = tokenizer.texts_to_sequences([' '.join(posts)])\n",
    "        seq_data = pad_sequences(sequences, maxlen=INPUT_LENGTH, padding='pre', truncating='post')\n",
    "        data[i] = seq_data\n",
    "    del inputs, alltexts\n",
    "    print('-----------{}-----------'.format(time.time() - start))\n",
    "    \n",
    "#     start = time.time()\n",
    "#     data = np.zeros((len(inputs), INPUT_LENGTH), dtype='int32')\n",
    "#     for i, posts in enumerate(inputs):\n",
    "#         ppp = []\n",
    "#         for j, post in enumerate(posts):\n",
    "#             pp = tokenizer.texts_to_sequences([post])\n",
    "#             if len(pp[0]) > 18:\n",
    "#                 ppp = ppp + pp[0][:18]\n",
    "#             else:\n",
    "#                 ppp = ppp + pp[0]\n",
    "#         sequences = [ppp]\n",
    "#         seq_data = pad_sequences(sequences, maxlen=INPUT_LENGTH, padding='pre', truncating='post')\n",
    "#         data[i] = seq_data\n",
    "#     del inputs, alltexts\n",
    "#     print('-----------{}-----------'.format(time.time() - start))\n",
    "\n",
    "    print('Finished loading data')\n",
    "    print('Shape of data tensor:', data.shape)\n",
    "    print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "    num_words = min(MAX_NB_WORDS, len(word_index) + 1)\n",
    "\n",
    "    print('Building embedding matrix')\n",
    "    print('%s Selected word tokens' % num_words)\n",
    "\n",
    "    start = time.time()\n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "    # embedding_matrix = np.random.uniform(-1, 1, (num_words, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= MAX_NB_WORDS:\n",
    "            break\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print('-----------{}-----------'.format(time.time() - start))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=12345)\n",
    "    for train_index, test_index in skf.split(np.asarray(data), labels[:, 1]):\n",
    "        np.random.seed(0)\n",
    "        tf.set_random_seed(0)\n",
    "        sess = tf.Session(graph=tf.get_default_graph())\n",
    "        K.set_session(sess)\n",
    "\n",
    "        print('--------------Start training----------------{}'.format(datetime.datetime.now()))\n",
    "        X_train, x_val = copy.deepcopy(data[train_index]), copy.deepcopy(data[test_index])\n",
    "        y_train, y_val = copy.deepcopy(labels[train_index]), copy.deepcopy(labels[test_index])\n",
    "#         del data, labels\n",
    "\n",
    "        print('Number of positive and negative classes in training and validation set')\n",
    "        print(y_train.sum(axis=0))\n",
    "        print(y_val.sum(axis=0))\n",
    "\n",
    "        class_weight = get_class_weights(np.asarray(y_train, 'int32')[:, 1])\n",
    "\n",
    "        FOLDER = \"{}/{}\".format(MODEL_FOLDER, datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M\"))\n",
    "        if not os.path.exists(FOLDER):\n",
    "            os.makedirs(FOLDER)\n",
    "        callbacks = TensorBoard(log_dir=FOLDER,\n",
    "                                write_graph=True, write_grads=False, histogram_freq=0,\n",
    "                                write_images=True, embeddings_freq=0, embeddings_layer_names='embedding_1',\n",
    "                                embeddings_metadata=None)\n",
    "\n",
    "        checkpoint = ModelCheckpoint(FOLDER + '/model.{epoch:02d}-{val_acc:.2f}.hdf5',\n",
    "                                     verbose=0, monitor='val_acc',\n",
    "                                     save_best_only=True, mode='auto')\n",
    "        \n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=15)\n",
    "\n",
    "        print(\"model fitting - Hierachical LSTM\")\n",
    "\n",
    "        print(class_weight)\n",
    "        model = create_dl_model()\n",
    "\n",
    "        model.fit(X_train, np.asarray(y_train, 'int32'),\n",
    "        # model.fit(X_train, np.asarray(y_train[:, 1], 'int32'),\n",
    "                  validation_data=(x_val, np.asarray(y_val, 'int32')),\n",
    "                  # validation_data=(x_val, np.asarray(y_val[:, 1], 'int32')),\n",
    "                  shuffle=False,\n",
    "                  nb_epoch=200, batch_size=32, verbose=0, class_weight=class_weight, callbacks=[checkpoint, callbacks, es])\n",
    "        \n",
    "        probas_ = model.predict(x_val)\n",
    "        df = pd.DataFrame(np.asarray(y_val[:, 1], 'int32'), columns=['label'])\n",
    "        df['class0'] = probas_[:, 0]\n",
    "        df['class1'] = probas_[:, 1]\n",
    "        df.to_csv('dl-' + str(INPUT_LENGTH) + '-' + str(EMBEDDING_DIM) + str(fold) + '.csv', index=False)\n",
    "        \n",
    "        model.save(FOLDER + '/my_model.h5')\n",
    "        del model, X_train, y_train, x_val, y_val, class_weight\n",
    "        fold += 1\n",
    "        K.clear_session()\n",
    "        print('--------------Finished training----------------'.format(datetime.datetime.now()))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 18, 100)           5000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 18, 100)           45600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 18, 100)           0         \n",
      "_________________________________________________________________\n",
      "attention_with_context_1 (At (None, 100)               10200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 5,056,002\n",
      "Trainable params: 56,002\n",
      "Non-trainable params: 5,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 2000, 18)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 2000, 2)           5056002   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 2000, 2)           8         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2000, 2)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 2000, 8)           192       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2000, 8)           0         \n",
      "_________________________________________________________________\n",
      "attention_with_context_2 (At (None, 8)                 80        \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 5,056,300\n",
      "Trainable params: 56,296\n",
      "Non-trainable params: 5,000,004\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\keras\\optimizers.py:890: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x2e25d0d7588>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "from utils import split_data\n",
    "from attention import AttentionWithContext\n",
    "from utils.users import load_users, select_post, combine_users\n",
    "from utils.utils import get_class_weights\n",
    "from utils.embed import load_glove_tw_vectors\n",
    "from utils.tokeniser import Tokenizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from keras.optimizers import Adam, AdaMod\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import Embedding, Dropout, Bidirectional, TimeDistributed\n",
    "from keras.layers import CuDNNGRU, CuDNNLSTM\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow import set_random_seed\n",
    "from keras.initializers import Constant\n",
    "\n",
    "\n",
    "def create_atte_model():\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                input_length=MAX_POST_LENGTH,\n",
    "                                weights=[embedding_matrix],\n",
    "                                trainable=False)\n",
    "\n",
    "    sequence_input = Input(shape=(MAX_POST_LENGTH,))\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    l_lstm_sent = Bidirectional(CuDNNGRU(50, return_sequences=True))(embedded_sequences)\n",
    "    l_lstm_sent = Dropout(0.5)(l_lstm_sent)\n",
    "    l_lstm_sent = AttentionWithContext()(l_lstm_sent)\n",
    "    l_lstm_sent = Dropout(0.5)(l_lstm_sent)\n",
    "    preds = Dense(units=2, activation='softmax')(l_lstm_sent)\n",
    "\n",
    "    sentEncoder = Model(sequence_input, preds)\n",
    "\n",
    "    print(sentEncoder.summary())\n",
    "\n",
    "    review_input = Input(shape=(MAX_POSTS, MAX_POST_LENGTH))\n",
    "    l_lstm_sent = TimeDistributed(sentEncoder)(review_input)\n",
    "    l_lstm_sent = BatchNormalization()(l_lstm_sent)\n",
    "    l_lstm_sent = Dropout(0.5)(l_lstm_sent)\n",
    "    l_lstm_sent = Bidirectional(CuDNNGRU(4, return_sequences=True))(l_lstm_sent)\n",
    "    l_lstm_sent = Dropout(0.5)(l_lstm_sent)\n",
    "    l_lstm_sent = AttentionWithContext()(l_lstm_sent)\n",
    "    l_lstm_sent = Dropout(0.5)(l_lstm_sent)\n",
    "    preds = Dense(2, activation='softmax')(l_lstm_sent)\n",
    "    model = Model(review_input, preds)\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    adam = AdaMod()\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['acc'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "num_words = 50000\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_POST_LENGTH = 18\n",
    "MAX_POSTS = 2000\n",
    "embedding_matrix = np.zeros((50000, 100))\n",
    "create_atte_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 18, 100)           5000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 18, 100)           45600     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 18, 100)           0         \n",
      "_________________________________________________________________\n",
      "attention_with_context_6 (At (None, 100)               10200     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 5,056,002\n",
      "Trainable params: 56,002\n",
      "Non-trainable params: 5,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 2000, 18)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 2000, 2)      5056002     input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 2000, 6)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 2000, 8)      0           time_distributed_4[0][0]         \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 2000, 8)      32          concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 2000, 8)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 2000, 32)     2496        dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 2000, 32)     0           bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_with_context_7 (Atten (None, 32)           1088        dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 32)           0           attention_with_context_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 2)            66          dropout_17[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 5,059,684\n",
      "Trainable params: 59,668\n",
      "Non-trainable params: 5,000,016\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x2e262cad788>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import concatenate\n",
    "def create_ana_model():\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                input_length=MAX_POST_LENGTH,\n",
    "                                weights=[embedding_matrix],\n",
    "                                trainable=False)\n",
    "\n",
    "    sequence_input = Input(shape=(MAX_POST_LENGTH,))\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    l_lstm_sent = Bidirectional(CuDNNGRU(50, return_sequences=True))(embedded_sequences)\n",
    "    l_lstm_sent = Dropout(0.5)(l_lstm_sent)\n",
    "    l_lstm_sent = AttentionWithContext()(l_lstm_sent)\n",
    "    l_lstm_sent = Dropout(0.5)(l_lstm_sent)\n",
    "    preds = Dense(units=2, activation='softmax')(l_lstm_sent)\n",
    "    sentEncoder = Model(sequence_input, preds)\n",
    "    print(sentEncoder.summary())\n",
    "\n",
    "    ana_input = Input(shape=(MAX_POSTS, len([1,2,3,4,5,6])))\n",
    "\n",
    "    review_input = Input(shape=(MAX_POSTS, MAX_POST_LENGTH))\n",
    "    l_lstm_sent = TimeDistributed(sentEncoder)(review_input)\n",
    "    l_lstm_sent = concatenate([l_lstm_sent, ana_input])\n",
    "    l_lstm_sent = BatchNormalization()(l_lstm_sent)\n",
    "    l_lstm_sent = Dropout(0.5)(l_lstm_sent)\n",
    "    l_lstm_sent = Bidirectional(CuDNNGRU(16, return_sequences=True))(l_lstm_sent)\n",
    "    l_lstm_sent = Dropout(0.5)(l_lstm_sent)\n",
    "    l_lstm_sent = AttentionWithContext()(l_lstm_sent)\n",
    "    l_lstm_sent = Dropout(0.5)(l_lstm_sent)\n",
    "    preds = Dense(2, activation='softmax')(l_lstm_sent)\n",
    "    model = Model([review_input, ana_input], preds)\n",
    "    print(model.summary())\n",
    "\n",
    "    adam = AdaMod()\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['acc'])\n",
    "\n",
    "    return model\n",
    "\n",
    "create_ana_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of control:  1699\n",
      "The number of depress:  1984\n",
      "Control shape:  (3944945, 3)\n",
      "Depress shape:  (4996621, 3)\n",
      "Selecting users with at least 100 posts\n",
      "Control users:  1699\n",
      "-----------11.66613483428955-----------\n",
      "Depress users:  1984\n",
      "-----------15.844536304473877-----------\n",
      "Labelling\n",
      "Loading embedding vectors from a file\n",
      "Total 1193514 word vectors.\n",
      "[ 780078428687065089            25567061           416684112\n",
      "           608725707  912541969968066560          2700680921\n",
      "          2329059770 1064934140007538688            43903078\n",
      "            16656127]\n",
      "                  tweetid   userid  \\\n",
      "3234  1143666788107866112  5882452   \n",
      "3235  1143666924246560768  5882452   \n",
      "3236  1143664692679720961  5882452   \n",
      "3237  1143661172891066370  5882452   \n",
      "3238  1143669737378893824  5882452   \n",
      "\n",
      "                                                  clean  label  \n",
      "3234  user user i've always felt like i was kind of ...      1  \n",
      "3235  user user i cannot function without my morning...      1  \n",
      "3236  user way too much of what you're describing so...      1  \n",
      "3237  user between chronic pain, anxiety, and depres...      1  \n",
      "3238  user user benadryl stops my itching but does n...      1  \n",
      "Selecting 3200 posts from each user\n",
      "-----------15.676273107528687-----------\n",
      "0    1699\n",
      "1     496\n",
      "Name: label, dtype: int64\n",
      "Input number:  2195\n",
      "Label number:  2195\n",
      "Tokenizing text\n",
      "Total 807701 unique tokens.\n",
      "-----------237.11562085151672-----------\n",
      "Transforming text to input sequences\n",
      "-----------179.94366669654846-----------\n",
      "Finished loading data\n",
      "Shape of data tensor: (2195, 35000)\n",
      "Shape of label tensor: (2195, 2)\n",
      "Building embedding matrix\n",
      "50000 Selected word tokens\n",
      "-----------0.11369681358337402-----------\n",
      "--------------Start training----------------2020-02-21 14:31:59.492226\n",
      "Number of positive and negative classes in training and validation set\n",
      "[1359.  397.]\n",
      "[340.  99.]\n",
      "model fitting - Hierachical LSTM\n",
      "{0: 1.0, 1: 3.4231738035264483}\n",
      "WARNING:tensorflow:From C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 35000)             0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 35000, 100)        5000000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 35000, 100)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 34998, 250)        75250     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 5,138,251\n",
      "Trainable params: 138,251\n",
      "Non-trainable params: 5,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\keras\\optimizers.py:890: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\ipykernel_launcher.py:197: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\keras\\callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\keras\\callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "--------------Start eva----------------\n",
      "Fold 1 accuracy = 93.39\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Control       0.94      0.98      0.96       340\n",
      "     Depress       0.91      0.79      0.84        99\n",
      "\n",
      "    accuracy                           0.93       439\n",
      "   macro avg       0.92      0.88      0.90       439\n",
      "weighted avg       0.93      0.93      0.93       439\n",
      "\n",
      "Fold 1 normal accuracy = 67.43\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Control       1.00      0.58      0.73       340\n",
      "     Depress       0.41      1.00      0.58        99\n",
      "\n",
      "    accuracy                           0.67       439\n",
      "   macro avg       0.70      0.79      0.66       439\n",
      "weighted avg       0.87      0.67      0.70       439\n",
      "\n",
      "-------------------------------------------Start Best Model--------------------------------------------\n",
      "Fold 1 accuracy = 85.42\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Control       0.88      0.94      0.91       340\n",
      "     Depress       0.73      0.57      0.64        99\n",
      "\n",
      "    accuracy                           0.85       439\n",
      "   macro avg       0.80      0.75      0.77       439\n",
      "weighted avg       0.85      0.85      0.85       439\n",
      "\n",
      "Fold 1 normal accuracy = 79.95\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Control       0.91      0.83      0.86       340\n",
      "     Depress       0.54      0.71      0.61        99\n",
      "\n",
      "    accuracy                           0.80       439\n",
      "   macro avg       0.72      0.77      0.74       439\n",
      "weighted avg       0.82      0.80      0.81       439\n",
      "\n",
      "--------------Finished training----------------\n",
      "[         2826065615          3960721393          4318344515\n",
      "          3382208403          2599342730          2192916252\n",
      "           181892265 1091181448843931649  836044275712475136\n",
      "            60688549]\n",
      "                  tweetid   userid  \\\n",
      "6479  1117414019164454912  6111852   \n",
      "6480  1117414158188929024  6111852   \n",
      "6481  1117413476631220226  6111852   \n",
      "6482  1116951765906227200  6111852   \n",
      "6483  1117037694268342272  6111852   \n",
      "\n",
      "                                                  clean  label  \n",
      "6479                     good morning, twitterverse url      1  \n",
      "6480  rt user: everything you need to know about k-p...      1  \n",
      "6481  it's also a rhetorical trick thwimp uses all t...      1  \n",
      "6482                          rt user: this thread: url      1  \n",
      "6483  putin has said america is no longer a leader. ...      1  \n",
      "Selecting 3200 posts from each user\n",
      "-----------17.02065134048462-----------\n",
      "0    1699\n",
      "1     496\n",
      "Name: label, dtype: int64\n",
      "Input number:  2195\n",
      "Label number:  2195\n",
      "Tokenizing text\n",
      "Total 804337 unique tokens.\n",
      "-----------221.69125699996948-----------\n",
      "Transforming text to input sequences\n",
      "-----------198.51977276802063-----------\n",
      "Finished loading data\n",
      "Shape of data tensor: (2195, 35000)\n",
      "Shape of label tensor: (2195, 2)\n",
      "Building embedding matrix\n",
      "50000 Selected word tokens\n",
      "-----------0.08676791191101074-----------\n",
      "--------------Start training----------------2020-02-21 14:42:40.148061\n",
      "Number of positive and negative classes in training and validation set\n",
      "[1359.  397.]\n",
      "[340.  99.]\n",
      "model fitting - Hierachical LSTM\n",
      "{0: 1.0, 1: 3.4231738035264483}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 35000)             0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 35000, 100)        5000000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 35000, 100)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 34998, 250)        75250     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 5,138,251\n",
      "Trainable params: 138,251\n",
      "Non-trainable params: 5,000,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\ipykernel_launcher.py:197: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Start eva----------------\n",
      "Fold 2 accuracy = 91.80\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Control       0.92      0.98      0.95       340\n",
      "     Depress       0.91      0.71      0.80        99\n",
      "\n",
      "    accuracy                           0.92       439\n",
      "   macro avg       0.91      0.84      0.87       439\n",
      "weighted avg       0.92      0.92      0.91       439\n",
      "\n",
      "Fold 2 normal accuracy = 70.62\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Control       0.99      0.63      0.77       340\n",
      "     Depress       0.43      0.98      0.60        99\n",
      "\n",
      "    accuracy                           0.71       439\n",
      "   macro avg       0.71      0.80      0.68       439\n",
      "weighted avg       0.86      0.71      0.73       439\n",
      "\n",
      "-------------------------------------------Start Best Model--------------------------------------------\n",
      "Fold 2 accuracy = 90.43\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Control       0.93      0.95      0.94       340\n",
      "     Depress       0.81      0.76      0.78        99\n",
      "\n",
      "    accuracy                           0.90       439\n",
      "   macro avg       0.87      0.85      0.86       439\n",
      "weighted avg       0.90      0.90      0.90       439\n",
      "\n",
      "Fold 2 normal accuracy = 78.82\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Control       0.98      0.74      0.84       340\n",
      "     Depress       0.52      0.96      0.67        99\n",
      "\n",
      "    accuracy                           0.79       439\n",
      "   macro avg       0.75      0.85      0.76       439\n",
      "weighted avg       0.88      0.79      0.80       439\n",
      "\n",
      "--------------Finished training----------------\n",
      "[           22936389          1586758399          3227968831\n",
      "          2731028615          3056177975            25787341\n",
      " 1104780414516977666           280932608            56891919\n",
      "  800940814641856512]\n",
      "               tweetid  userid  \\\n",
      "0   589962698353299457  802310   \n",
      "1  1130666193289863168  802310   \n",
      "2  1070479497898520577  802310   \n",
      "3  1070482628153794560  802310   \n",
      "4  1070483237879799809  802310   \n",
      "\n",
      "                                               clean  label  \n",
      "0  user github is giving me more grief than the l...      1  \n",
      "1  user some of us just ride the risk-train and g...      1  \n",
      "2  user super happy to see another competent deve...      1  \n",
      "3  user you can't trust lnp to make sensible cybe...      1  \n",
      "4  user user is user prepared to stand against <h...      1  \n",
      "Selecting 3200 posts from each user\n",
      "-----------16.744054317474365-----------\n",
      "0    1699\n",
      "1     496\n",
      "Name: label, dtype: int64\n",
      "Input number:  2195\n",
      "Label number:  2195\n",
      "Tokenizing text\n",
      "Total 793449 unique tokens.\n",
      "-----------222.37827849388123-----------\n",
      "Transforming text to input sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3319, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-99bddbfae5c0>\", line 125, in <module>\n",
      "    sequences = tokenizer.texts_to_sequences([' '.join(posts)])\n",
      "  File \"C:\\Users\\Bright\\PycharmProjects\\final_thesis\\utils\\tokeniser.py\", line 86, in texts_to_sequences\n",
      "    seq.append(self.twk.tokenize(text.lower()))\n",
      "  File \"C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\nltk\\tokenize\\casual.py\", line 301, in tokenize\n",
      "    if not self.preserve_case:\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2034, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1151, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.random import seed\n",
    "from utils import split_data\n",
    "from utils.tokeniser import Tokenizer\n",
    "from utils.utils import get_class_weights\n",
    "from utils.embed import load_glove_tw_vectors\n",
    "from utils.users import load_users, select_post, combine_users\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from keras.optimizers import Adam, AdaMod\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers import Embedding, Dropout, Input\n",
    "from keras.models import Model\n",
    "from tensorflow import set_random_seed\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from utils.metrics import get_max_acc, find_optimal_cutoff\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import glob\n",
    "\n",
    "\n",
    "def create_dl_model():\n",
    "    num_filters = 250\n",
    "    drop = 0.5\n",
    "\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                input_length=INPUT_LENGTH,\n",
    "                                weights=[embedding_matrix],\n",
    "                                trainable=False)\n",
    "                                # trainable=True)\n",
    "\n",
    "    model_input = Input(shape=(INPUT_LENGTH,), dtype='int32')\n",
    "    model_layer = embedding_layer(model_input)\n",
    "    model_layer = Dropout(drop)(model_layer)\n",
    "    model_layer = Conv1D(filters=num_filters, kernel_size=3)(model_layer)\n",
    "    model_layer = GlobalMaxPooling1D()(model_layer)\n",
    "    # model_layer = Dropout(drop)(model_layer)\n",
    "    model_layer = Dense(num_filters, activation='relu')(model_layer)\n",
    "    model_layer = Dropout(drop)(model_layer)\n",
    "    # model_layer = Dense(2, activation='softmax')(model_layer)\n",
    "    model_layer = Dense(1, activation='sigmoid')(model_layer)\n",
    "    model = Model(model_input, model_layer)\n",
    "\n",
    "    print(model.summary())\n",
    "    adam = Adam(lr=0.002)\n",
    "    # adam = AdaMod()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])  # metrics.binary_accuracy\n",
    "    return model\n",
    "\n",
    "\n",
    "SEED = 1\n",
    "seed(SEED)\n",
    "set_random_seed(SEED)\n",
    "\n",
    "POST_SIZE = 100\n",
    "MAX_POSTS = 3200\n",
    "MAX_NB_WORDS = 50000\n",
    "INPUT_LENGTH = 35000\n",
    "EMBEDDING_DIM = 100\n",
    "NB_DEPRESS = 1984\n",
    "\n",
    "DIR = '../data/'\n",
    "\n",
    "inputs, labels = load_users(DIR, POST_SIZE, MAX_POSTS, NB_DEPRESS, True)\n",
    "control = inputs[inputs.label == 0].reset_index(drop=True).copy()\n",
    "depress = inputs[inputs.label == 1].reset_index(drop=True).copy()\n",
    "del inputs, labels\n",
    "\n",
    "embeddings_index = load_glove_tw_vectors(EMBEDDING_DIM)  # load pre-trained embedding vectors\n",
    "\n",
    "i = 0\n",
    "fold = 0\n",
    "results = []\n",
    "MODEL_FOLDER = \"logs/{}/{}\".format('DL', datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M\") + '-' + str(MAX_NB_WORDS) + '-' + str(INPUT_LENGTH))\n",
    "for data_index in split_data.split(depress, 4):\n",
    "    data_fold = depress[depress.userid.isin(data_index)].copy()\n",
    "\n",
    "    print(data_index[:10])\n",
    "    print(data_fold.head())\n",
    "\n",
    "    combine, labels = combine_users(control, data_fold)\n",
    "    inputs, labels = select_post(combine, labels, MAX_POSTS)\n",
    "    del combine, data_fold\n",
    "\n",
    "    # transform Y to categories\n",
    "    print(labels.label.value_counts())\n",
    "    labels = labels.label.values\n",
    "    labels = to_categorical(np.asarray(labels))\n",
    "\n",
    "    print('Input number: ', len(inputs))\n",
    "    print('Label number: ', len(labels))\n",
    "\n",
    "    # alltexts = np.hstack(np.array(inputs))\n",
    "    alltexts = np.hstack(np.array(inputs).flatten())\n",
    "\n",
    "    print('Tokenizing text')\n",
    "    start = time.time()\n",
    "    tknzr = TweetTokenizer(reduce_len=True)\n",
    "    alltexts = [tknzr.tokenize(text.lower()) for text in alltexts]\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(alltexts)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Total %s unique tokens.' % len(word_index))\n",
    "    print('-----------{}-----------'.format(time.time() - start))\n",
    "\n",
    "    print('Transforming text to input sequences')\n",
    "    start = time.time()\n",
    "    data = np.zeros((len(inputs), INPUT_LENGTH), dtype='int32')\n",
    "    for i, posts in enumerate(inputs):\n",
    "        sequences = tokenizer.texts_to_sequences([' '.join(posts)])\n",
    "        # sequences = tokenizer.lists_to_sequences([np.hstack(np.array(posts))])\n",
    "        seq_data = pad_sequences(sequences, maxlen=INPUT_LENGTH, padding='pre', truncating='post')\n",
    "        data[i] = seq_data\n",
    "    del inputs, alltexts\n",
    "    print('-----------{}-----------'.format(time.time() - start))\n",
    "\n",
    "    print('Finished loading data')\n",
    "    print('Shape of data tensor:', data.shape)\n",
    "    print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "    num_words = min(MAX_NB_WORDS, len(word_index) + 1)\n",
    "\n",
    "    print('Building embedding matrix')\n",
    "    print('%s Selected word tokens' % num_words)\n",
    "\n",
    "    if 'embeddings_index' in globals():\n",
    "        start = time.time()\n",
    "        embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "        # embedding_matrix = np.random.uniform(-1, 1, (num_words, EMBEDDING_DIM))\n",
    "        for word, i in word_index.items():\n",
    "            if i >= MAX_NB_WORDS:\n",
    "                break\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        print('-----------{}-----------'.format(time.time() - start))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=12345)\n",
    "    for train_index, test_index in skf.split(np.asarray(data), labels[:, 1]):\n",
    "        np.random.seed(0)\n",
    "        tf.set_random_seed(0)\n",
    "        sess = tf.Session(graph=tf.get_default_graph())\n",
    "        K.set_session(sess)\n",
    "\n",
    "        print('--------------Start training----------------{}'.format(datetime.datetime.now()))\n",
    "        X_train, x_val = copy.deepcopy(data[train_index]), copy.deepcopy(data[test_index])\n",
    "        y_train, y_val = copy.deepcopy(labels[train_index]), copy.deepcopy(labels[test_index])\n",
    "        del data, labels\n",
    "\n",
    "        print('Number of positive and negative classes in training and validation set')\n",
    "        print(y_train.sum(axis=0))\n",
    "        print(y_val.sum(axis=0))\n",
    "\n",
    "        class_weight = get_class_weights(np.asarray(y_train, 'int32')[:, 1])\n",
    "\n",
    "        FOLDER = \"{}/{}\".format(MODEL_FOLDER, datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M\"))\n",
    "        if not os.path.exists(FOLDER):\n",
    "            os.makedirs(FOLDER)\n",
    "        callbacks = TensorBoard(log_dir=FOLDER,\n",
    "                                write_graph=True, write_grads=False, histogram_freq=0,\n",
    "                                write_images=True, embeddings_freq=0, embeddings_layer_names='embedding_1',\n",
    "                                embeddings_metadata=None)\n",
    "\n",
    "        checkpoint = ModelCheckpoint(FOLDER + '/model.{epoch:02d}-{val_acc:.2f}.hdf5',\n",
    "                                     verbose=0, monitor='val_acc',\n",
    "                                     save_best_only=True, mode='auto')\n",
    "\n",
    "        es = EarlyStopping(monitor='val_acc', mode='auto', verbose=0, patience=10)\n",
    "\n",
    "        print(\"model fitting - Hierachical LSTM\")\n",
    "\n",
    "        print(class_weight)\n",
    "        model = create_dl_model()\n",
    "\n",
    "        # model.fit(X_train, np.asarray(y_train, 'int32'),\n",
    "        model.fit(X_train, np.asarray(y_train[:, 1], 'int32'),\n",
    "                  # validation_data=(x_val, np.asarray(y_val, 'int32')),\n",
    "                  validation_data=(x_val, np.asarray(y_val[:, 1], 'int32')),\n",
    "                  shuffle=False,\n",
    "                  nb_epoch=200, batch_size=32, verbose=0, class_weight=class_weight,\n",
    "                  callbacks=[checkpoint, callbacks, es])\n",
    "\n",
    "        model.save(FOLDER + '/my_model.h5')\n",
    "\n",
    "        print('--------------Start eva----------------'.format(datetime.datetime.now()))\n",
    "        model_name = sorted(glob.iglob(os.path.join(FOLDER, '*.h5')), key=os.path.getctime, reverse=True)[0]\n",
    "        model = load_model(model_name,\n",
    "                           custom_objects={'optimizer': AdaMod})\n",
    "        probas_ = model.predict(x_val)\n",
    "        y_test = y_val[:, 1].astype(int)\n",
    "#         max_accuracy, thresh = get_max_acc(y_test, probas_[:, 1])\n",
    "#         pred = [1 if m > thresh else 0 for m in probas_[:, 1]]\n",
    "        max_accuracy, thresh = get_max_acc(y_test, probas_)\n",
    "        pred = [1 if m > thresh else 0 for m in probas_]\n",
    "        print('Fold {} accuracy = {:.2f}'.format(fold + 1, max_accuracy * 100))\n",
    "        print(classification_report(y_test, pred, target_names=['Control', 'Depress']))\n",
    "\n",
    "        df = pd.DataFrame(y_test, columns=['label'])\n",
    "#         df['class0'] = probas_[:, 0]\n",
    "        df['class1'] = probas_\n",
    "        df.to_csv(MODEL_FOLDER + '/dl-' + str(INPUT_LENGTH) + '-' + str(EMBEDDING_DIM) + str(fold) + '.csv', index=False)\n",
    "\n",
    "        weighted = precision_recall_fscore_support(y_test, pred, average='weighted')\n",
    "        macro = precision_recall_fscore_support(y_test, pred, average='macro')\n",
    "        micro = precision_recall_fscore_support(y_test, pred, average='micro')\n",
    "        both = precision_recall_fscore_support(y_test, pred)\n",
    "        results.append(['dl-max-' + str(fold),\n",
    "                        max_accuracy, weighted[0], weighted[1], weighted[2],\n",
    "                        macro[0], macro[1], macro[2], micro[0], micro[1], micro[2],\n",
    "                        both[0][0], both[1][0], both[2][0], both[0][1], both[1][1], both[2][1]])\n",
    "\n",
    "        pred = [1 if m > 0.5 else 0 for m in probas_]\n",
    "        print('Fold {} normal accuracy = {:.2f}'.format(fold + 1, accuracy_score(y_test, pred) * 100))\n",
    "        max_accuracy = accuracy_score(y_test, pred)\n",
    "        print(classification_report(y_test, pred, target_names=['Control', 'Depress']))\n",
    "\n",
    "        weighted = precision_recall_fscore_support(y_test, pred, average='weighted')\n",
    "        macro = precision_recall_fscore_support(y_test, pred, average='macro')\n",
    "        micro = precision_recall_fscore_support(y_test, pred, average='micro')\n",
    "        both = precision_recall_fscore_support(y_test, pred)\n",
    "        results.append(['dl-nor-' + str(fold),\n",
    "                        max_accuracy, weighted[0], weighted[1], weighted[2],\n",
    "                        macro[0], macro[1], macro[2], micro[0], micro[1], micro[2],\n",
    "                        both[0][0], both[1][0], both[2][0], both[0][1], both[1][1], both[2][1]])\n",
    "\n",
    "        print('-------------------------------------------Start Best Model--------------------------------------------')\n",
    "\n",
    "        model_name = sorted(glob.iglob(os.path.join(FOLDER, '*.hdf5')), key=os.path.getctime, reverse=True)[0]\n",
    "        model = load_model(model_name,\n",
    "                           custom_objects={'optimizer': AdaMod})\n",
    "        probas_ = model.predict(x_val)\n",
    "        y_test = y_val[:, 1].astype(int)\n",
    "#         max_accuracy, thresh = get_max_acc(y_test, probas_[:, 1])\n",
    "#         pred = [1 if m > thresh else 0 for m in probas_[:, 1]]\n",
    "        max_accuracy, thresh = get_max_acc(y_test, probas_)\n",
    "        pred = [1 if m > thresh else 0 for m in probas_]\n",
    "        print('Fold {} accuracy = {:.2f}'.format(fold + 1, max_accuracy * 100))\n",
    "        print(classification_report(y_test, pred, target_names=['Control', 'Depress']))\n",
    "\n",
    "        df = pd.DataFrame(y_test, columns=['label'])\n",
    "#         df['class0'] = probas_[:, 0]\n",
    "        df['class1'] = probas_\n",
    "        df.to_csv(MODEL_FOLDER + '/dl-best-' + str(INPUT_LENGTH) + '-' + str(EMBEDDING_DIM) + str(fold) + '.csv', index=False)\n",
    "\n",
    "        weighted = precision_recall_fscore_support(y_test, pred, average='weighted')\n",
    "        macro = precision_recall_fscore_support(y_test, pred, average='macro')\n",
    "        micro = precision_recall_fscore_support(y_test, pred, average='micro')\n",
    "        both = precision_recall_fscore_support(y_test, pred)\n",
    "        results.append(['dl-best-max-' + str(fold),\n",
    "                        max_accuracy, weighted[0], weighted[1], weighted[2],\n",
    "                        macro[0], macro[1], macro[2], micro[0], micro[1], micro[2],\n",
    "                        both[0][0], both[1][0], both[2][0], both[0][1], both[1][1], both[2][1]])\n",
    "\n",
    "        pred = [1 if m > 0.5 else 0 for m in probas_]\n",
    "        print('Fold {} normal accuracy = {:.2f}'.format(fold + 1, accuracy_score(y_test, pred) * 100))\n",
    "        max_accuracy = accuracy_score(y_test, pred)\n",
    "        print(classification_report(y_test, pred, target_names=['Control', 'Depress']))\n",
    "\n",
    "        weighted = precision_recall_fscore_support(y_test, pred, average='weighted')\n",
    "        macro = precision_recall_fscore_support(y_test, pred, average='macro')\n",
    "        micro = precision_recall_fscore_support(y_test, pred, average='micro')\n",
    "        both = precision_recall_fscore_support(y_test, pred)\n",
    "        results.append(['dl-best-nor-' + str(fold),\n",
    "                        max_accuracy, weighted[0], weighted[1], weighted[2],\n",
    "                        macro[0], macro[1], macro[2], micro[0], micro[1], micro[2],\n",
    "                        both[0][0], both[1][0], both[2][0], both[0][1], both[1][1], both[2][1]])\n",
    "\n",
    "        del model, X_train, y_train, x_val, y_val, class_weight\n",
    "        K.clear_session()\n",
    "        fold += 1\n",
    "        print('--------------Finished training----------------'.format(datetime.datetime.now()))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of control:  1699\n",
      "The number of depress:  1984\n",
      "Control shape:  (3944945, 3)\n",
      "Depress shape:  (4996621, 3)\n",
      "Selecting users with at least 100 posts\n",
      "Control users:  1699\n",
      "-----------13.731928825378418-----------\n",
      "Depress users:  1984\n",
      "-----------21.492454290390015-----------\n",
      "Labelling\n",
      "[ 780078428687065089            25567061           416684112\n",
      "           608725707  912541969968066560          2700680921\n",
      "          2329059770 1064934140007538688            43903078\n",
      "            16656127]\n",
      "                  tweetid   userid  \\\n",
      "3234  1143666788107866112  5882452   \n",
      "3235  1143666924246560768  5882452   \n",
      "3236  1143664692679720961  5882452   \n",
      "3237  1143661172891066370  5882452   \n",
      "3238  1143669737378893824  5882452   \n",
      "\n",
      "                                                  clean  label  \n",
      "3234  user user i've always felt like i was kind of ...      1  \n",
      "3235  user user i cannot function without my morning...      1  \n",
      "3236  user way too much of what you're describing so...      1  \n",
      "3237  user between chronic pain, anxiety, and depres...      1  \n",
      "3238  user user benadryl stops my itching but does n...      1  \n",
      "Selecting 3200 posts from each user\n",
      "-----------19.48834538459778-----------\n",
      "0    1699\n",
      "1     496\n",
      "Name: label, dtype: int64\n",
      "Input number:  2195\n",
      "Label number:  2195\n",
      "Tokenizing text\n",
      "Total 807701 unique tokens.\n",
      "-----------241.61453223228455-----------\n",
      "Transforming text to input sequences\n",
      "-----------252.31186032295227-----------\n",
      "Finished loading data\n",
      "Shape of data tensor: (2195, 35000)\n",
      "Shape of label tensor: (2195, 2)\n",
      "Building embedding matrix\n",
      "50000 Selected word tokens\n",
      "-----------0.3585944175720215-----------\n",
      "--------------Start training----------------2020-02-22 12:00:23.967647\n"
     ]
    }
   ],
   "source": [
    "SEED = 1\n",
    "seed(SEED)\n",
    "set_random_seed(SEED)\n",
    "\n",
    "POST_SIZE = 100\n",
    "MAX_POSTS = 3200\n",
    "MAX_NB_WORDS = 50000\n",
    "INPUT_LENGTH = 35000\n",
    "EMBEDDING_DIM = 100\n",
    "NB_DEPRESS = 1984\n",
    "\n",
    "DIR = '../data/'\n",
    "\n",
    "inputs, labels = load_users(DIR, POST_SIZE, MAX_POSTS, NB_DEPRESS, True)\n",
    "control = inputs[inputs.label == 0].reset_index(drop=True).copy()\n",
    "depress = inputs[inputs.label == 1].reset_index(drop=True).copy()\n",
    "del inputs, labels\n",
    "\n",
    "i = 0\n",
    "fold = 0\n",
    "results = []\n",
    "MODEL_FOLDER = \"logs/{}/{}\".format('DL', datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M\") + '-' + str(MAX_NB_WORDS) + '-' + str(INPUT_LENGTH))\n",
    "for data_index in split_data.split(depress, 4):\n",
    "    data_fold = depress[depress.userid.isin(data_index)].copy()\n",
    "\n",
    "    print(data_index[:10])\n",
    "    print(data_fold.head())\n",
    "\n",
    "    combine, labels = combine_users(control, data_fold)\n",
    "    inputs, labels = select_post(combine, labels, MAX_POSTS)\n",
    "    del combine, data_fold\n",
    "\n",
    "    # transform Y to categories\n",
    "    print(labels.label.value_counts())\n",
    "    labels = labels.label.values\n",
    "    labels = to_categorical(np.asarray(labels))\n",
    "\n",
    "    print('Input number: ', len(inputs))\n",
    "    print('Label number: ', len(labels))\n",
    "\n",
    "    # alltexts = np.hstack(np.array(inputs))\n",
    "    alltexts = np.hstack(np.array(inputs).flatten())\n",
    "\n",
    "    print('Tokenizing text')\n",
    "    start = time.time()\n",
    "    tknzr = TweetTokenizer(reduce_len=True)\n",
    "    alltexts = [tknzr.tokenize(text.lower()) for text in alltexts]\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(alltexts)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Total %s unique tokens.' % len(word_index))\n",
    "    print('-----------{}-----------'.format(time.time() - start))\n",
    "\n",
    "    print('Transforming text to input sequences')\n",
    "    start = time.time()\n",
    "    data = np.zeros((len(inputs), INPUT_LENGTH), dtype='int32')\n",
    "    for i, posts in enumerate(inputs):\n",
    "        sequences = tokenizer.texts_to_sequences([' '.join(posts)])\n",
    "        seq_data = pad_sequences(sequences, maxlen=INPUT_LENGTH, padding='pre', truncating='post')\n",
    "        data[i] = seq_data\n",
    "    del inputs, alltexts\n",
    "    print('-----------{}-----------'.format(time.time() - start))\n",
    "\n",
    "    print('Finished loading data')\n",
    "    print('Shape of data tensor:', data.shape)\n",
    "    print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "    num_words = min(MAX_NB_WORDS, len(word_index) + 1)\n",
    "\n",
    "    print('Building embedding matrix')\n",
    "    print('%s Selected word tokens' % num_words)\n",
    "\n",
    "    if 'embeddings_index' in globals():\n",
    "        start = time.time()\n",
    "        embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "        # embedding_matrix = np.random.uniform(-1, 1, (num_words, EMBEDDING_DIM))\n",
    "        for word, i in word_index.items():\n",
    "            if i >= MAX_NB_WORDS:\n",
    "                break\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        print('-----------{}-----------'.format(time.time() - start))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=12345)\n",
    "    for train_index, test_index in skf.split(np.asarray(data), labels[:, 1]):\n",
    "        np.random.seed(0)\n",
    "        tf.set_random_seed(0)\n",
    "        sess = tf.Session(graph=tf.get_default_graph())\n",
    "        K.set_session(sess)\n",
    "\n",
    "        print('--------------Start training----------------{}'.format(datetime.datetime.now()))\n",
    "        X_train, x_val = copy.deepcopy(data[train_index]), copy.deepcopy(data[test_index])\n",
    "        y_train, y_val = copy.deepcopy(labels[train_index]), copy.deepcopy(labels[test_index])\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Hierachical LSTM\n",
      "{0: 1.0, 1: 3.4231738035264483}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 35000, 100)        5000000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 35000, 100)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 34998, 250)        75250     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 502       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 5,138,502\n",
      "Trainable params: 138,502\n",
      "Non-trainable params: 5,000,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\ipykernel_launcher.py:78: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Start eva----------------\n",
      "Fold 1 accuracy = 77.45\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Control       0.77      1.00      0.87       340\n",
      "     Depress       0.00      0.00      0.00        99\n",
      "\n",
      "    accuracy                           0.77       439\n",
      "   macro avg       0.39      0.50      0.44       439\n",
      "weighted avg       0.60      0.77      0.68       439\n",
      "\n",
      "Fold 1 normal accuracy = 77.45\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Control       0.77      1.00      0.87       340\n",
      "     Depress       0.00      0.00      0.00        99\n",
      "\n",
      "    accuracy                           0.77       439\n",
      "   macro avg       0.39      0.50      0.44       439\n",
      "weighted avg       0.60      0.77      0.68       439\n",
      "\n",
      "-------------------------------------------Start Best Model--------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 accuracy = 77.45\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Control       0.77      1.00      0.87       340\n",
      "     Depress       0.00      0.00      0.00        99\n",
      "\n",
      "    accuracy                           0.77       439\n",
      "   macro avg       0.39      0.50      0.44       439\n",
      "weighted avg       0.60      0.77      0.68       439\n",
      "\n",
      "Fold 1 normal accuracy = 77.45\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Control       0.77      1.00      0.87       340\n",
      "     Depress       0.00      0.00      0.00        99\n",
      "\n",
      "    accuracy                           0.77       439\n",
      "   macro avg       0.39      0.50      0.44       439\n",
      "weighted avg       0.60      0.77      0.68       439\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "\n",
    "def create_dl_model():\n",
    "    num_filters = 250\n",
    "    drop = 0.2\n",
    "\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                input_length=INPUT_LENGTH,\n",
    "                                weights=[embedding_matrix],\n",
    "                                trainable=False)\n",
    "                                # trainable=True)\n",
    "\n",
    "#     model_input = Input(shape=(INPUT_LENGTH,), dtype='int32')\n",
    "#     model_layer = embedding_layer(model_input)\n",
    "#     model_layer = Dropout(drop)(model_layer)\n",
    "#     model_layer = Conv1D(filters=num_filters, kernel_size=3)(model_layer)\n",
    "#     model_layer = GlobalMaxPooling1D()(model_layer)\n",
    "#     model_layer = Dense(num_filters, activation='relu')(model_layer)\n",
    "#     model_layer = Dropout(drop)(model_layer)\n",
    "#     model_layer = Dense(2, activation='softmax')(model_layer)\n",
    "#     model = Model(model_input, model_layer)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words,\n",
    "                        EMBEDDING_DIM,\n",
    "                        input_length=INPUT_LENGTH,\n",
    "                        weights=[embedding_matrix],\n",
    "                                trainable=False))\n",
    "#                         trainable=True))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Conv1D(filters=num_filters, kernel_size=3))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(250))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    print(model.summary())\n",
    "    adam = Adam(lr=0.004, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    # adam = AdaMod()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'])  # metrics.binary_accuracy\n",
    "    return model\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "K.set_session(sess)\n",
    "\n",
    "class_weight = get_class_weights(np.asarray(y_train, 'int32')[:, 1])\n",
    "\n",
    "FOLDER = \"{}/{}\".format(MODEL_FOLDER, datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M\"))\n",
    "if not os.path.exists(FOLDER):\n",
    "    os.makedirs(FOLDER)\n",
    "callbacks = TensorBoard(log_dir=FOLDER,\n",
    "                        write_graph=True, write_grads=False, histogram_freq=0,\n",
    "                        write_images=True, embeddings_freq=0, embeddings_layer_names='embedding_1',\n",
    "                        embeddings_metadata=None)\n",
    "\n",
    "checkpoint = ModelCheckpoint(FOLDER + '/model.{epoch:02d}-{val_acc:.2f}.hdf5',\n",
    "                             verbose=0, monitor='val_acc',\n",
    "                             save_best_only=True, mode='auto')\n",
    "\n",
    "es = EarlyStopping(monitor='val_acc', mode='auto', verbose=0, patience=10)\n",
    "\n",
    "print(\"model fitting - Hierachical LSTM\")\n",
    "\n",
    "print(class_weight)\n",
    "model = create_dl_model()\n",
    "\n",
    "model.fit(X_train, np.asarray(y_train, 'int32'),\n",
    "# model.fit(X_train, np.asarray(y_train[:, 1], 'int32'),\n",
    "          validation_data=(x_val, np.asarray(y_val, 'int32')),\n",
    "#           validation_data=(x_val, np.asarray(y_val[:, 1], 'int32')),\n",
    "          shuffle=False, #class_weight=class_weight,\n",
    "          nb_epoch=200, batch_size=32, verbose=0, \n",
    "          callbacks=[checkpoint, callbacks, es])\n",
    "\n",
    "model.save(FOLDER + '/my_model.h5')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('--------------Start eva----------------'.format(datetime.datetime.now()))\n",
    "model_name = sorted(glob.iglob(os.path.join(FOLDER, '*.h5')), key=os.path.getctime, reverse=True)[0]\n",
    "model = load_model(model_name,\n",
    "                   custom_objects={'optimizer': AdaMod})\n",
    "probas_ = model.predict(x_val)\n",
    "y_test = y_val[:, 1].astype(int)\n",
    "max_accuracy, thresh = get_max_acc(y_test, probas_[:, 1])\n",
    "pred = [1 if m > thresh else 0 for m in probas_[:, 1]]\n",
    "print('Fold {} accuracy = {:.2f}'.format(fold + 1, max_accuracy * 100))\n",
    "print(classification_report(y_test, pred, target_names=['Control', 'Depress']))\n",
    "\n",
    "df = pd.DataFrame(y_test, columns=['label'])\n",
    "df['class0'] = probas_[:, 0]\n",
    "df['class1'] = probas_[:, 1]\n",
    "df.to_csv(MODEL_FOLDER + '/dl-' + str(INPUT_LENGTH) + '-' + str(EMBEDDING_DIM) + str(fold) + '.csv', index=False)\n",
    "\n",
    "weighted = precision_recall_fscore_support(y_test, pred, average='weighted')\n",
    "macro = precision_recall_fscore_support(y_test, pred, average='macro')\n",
    "micro = precision_recall_fscore_support(y_test, pred, average='micro')\n",
    "both = precision_recall_fscore_support(y_test, pred)\n",
    "results.append(['dl-max-' + str(fold),\n",
    "                max_accuracy, weighted[0], weighted[1], weighted[2],\n",
    "                macro[0], macro[1], macro[2], micro[0], micro[1], micro[2],\n",
    "                both[0][0], both[1][0], both[2][0], both[0][1], both[1][1], both[2][1]])\n",
    "\n",
    "pred = [1 if m > 0.5 else 0 for m in probas_[:, 1]]\n",
    "print('Fold {} normal accuracy = {:.2f}'.format(fold + 1, accuracy_score(y_test, pred) * 100))\n",
    "max_accuracy = accuracy_score(y_test, pred)\n",
    "print(classification_report(y_test, pred, target_names=['Control', 'Depress']))\n",
    "\n",
    "weighted = precision_recall_fscore_support(y_test, pred, average='weighted')\n",
    "macro = precision_recall_fscore_support(y_test, pred, average='macro')\n",
    "micro = precision_recall_fscore_support(y_test, pred, average='micro')\n",
    "both = precision_recall_fscore_support(y_test, pred)\n",
    "results.append(['dl-nor-' + str(fold),\n",
    "                max_accuracy, weighted[0], weighted[1], weighted[2],\n",
    "                macro[0], macro[1], macro[2], micro[0], micro[1], micro[2],\n",
    "                both[0][0], both[1][0], both[2][0], both[0][1], both[1][1], both[2][1]])\n",
    "\n",
    "print('-------------------------------------------Start Best Model--------------------------------------------')\n",
    "\n",
    "model_name = sorted(glob.iglob(os.path.join(FOLDER, '*.hdf5')), key=os.path.getctime, reverse=True)[0]\n",
    "model = load_model(model_name,\n",
    "                   custom_objects={'optimizer': AdaMod})\n",
    "probas_ = model.predict(x_val)\n",
    "y_test = y_val[:, 1].astype(int)\n",
    "max_accuracy, thresh = get_max_acc(y_test, probas_[:, 1])\n",
    "pred = [1 if m > thresh else 0 for m in probas_[:, 1]]\n",
    "print('Fold {} accuracy = {:.2f}'.format(fold + 1, max_accuracy * 100))\n",
    "print(classification_report(y_test, pred, target_names=['Control', 'Depress']))\n",
    "\n",
    "df = pd.DataFrame(y_test, columns=['label'])\n",
    "df['class0'] = probas_[:, 0]\n",
    "df['class1'] = probas_[:, 1]\n",
    "df.to_csv(MODEL_FOLDER + '/dl-best-' + str(INPUT_LENGTH) + '-' + str(EMBEDDING_DIM) + str(fold) + '.csv', index=False)\n",
    "\n",
    "weighted = precision_recall_fscore_support(y_test, pred, average='weighted')\n",
    "macro = precision_recall_fscore_support(y_test, pred, average='macro')\n",
    "micro = precision_recall_fscore_support(y_test, pred, average='micro')\n",
    "both = precision_recall_fscore_support(y_test, pred)\n",
    "results.append(['dl-best-max-' + str(fold),\n",
    "                max_accuracy, weighted[0], weighted[1], weighted[2],\n",
    "                macro[0], macro[1], macro[2], micro[0], micro[1], micro[2],\n",
    "                both[0][0], both[1][0], both[2][0], both[0][1], both[1][1], both[2][1]])\n",
    "\n",
    "pred = [1 if m > 0.5 else 0 for m in probas_[:, 1]]\n",
    "print('Fold {} normal accuracy = {:.2f}'.format(fold + 1, accuracy_score(y_test, pred) * 100))\n",
    "max_accuracy = accuracy_score(y_test, pred)\n",
    "print(classification_report(y_test, pred, target_names=['Control', 'Depress']))\n",
    "\n",
    "weighted = precision_recall_fscore_support(y_test, pred, average='weighted')\n",
    "macro = precision_recall_fscore_support(y_test, pred, average='macro')\n",
    "micro = precision_recall_fscore_support(y_test, pred, average='micro')\n",
    "both = precision_recall_fscore_support(y_test, pred)\n",
    "results.append(['dl-best-nor-' + str(fold),\n",
    "                max_accuracy, weighted[0], weighted[1], weighted[2],\n",
    "                macro[0], macro[1], macro[2], micro[0], micro[1], micro[2],\n",
    "                both[0][0], both[1][0], both[2][0], both[0][1], both[1][1], both[2][1]])\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Hierachical LSTM\n",
      "{0: 1.0, 1: 3.4231738035264483}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 15000, 100)        5000000   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 15000, 100)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 14998, 250)        75250     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 502       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 5,138,502\n",
      "Trainable params: 138,502\n",
      "Non-trainable params: 5,000,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\ipykernel_launcher.py:90: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Start eva----------------\n",
      "Fold 1 accuracy = 77.45\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Control       0.77      1.00      0.87       340\n",
      "     Depress       0.00      0.00      0.00        99\n",
      "\n",
      "    accuracy                           0.77       439\n",
      "   macro avg       0.39      0.50      0.44       439\n",
      "weighted avg       0.60      0.77      0.68       439\n",
      "\n",
      "Fold 1 normal accuracy = 77.45\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Control       0.77      1.00      0.87       340\n",
      "     Depress       0.00      0.00      0.00        99\n",
      "\n",
      "    accuracy                           0.77       439\n",
      "   macro avg       0.39      0.50      0.44       439\n",
      "weighted avg       0.60      0.77      0.68       439\n",
      "\n",
      "-------------------------------------------Start Best Model--------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 accuracy = 77.45\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Control       0.77      1.00      0.87       340\n",
      "     Depress       0.00      0.00      0.00        99\n",
      "\n",
      "    accuracy                           0.77       439\n",
      "   macro avg       0.39      0.50      0.44       439\n",
      "weighted avg       0.60      0.77      0.68       439\n",
      "\n",
      "Fold 1 normal accuracy = 77.45\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Control       0.77      1.00      0.87       340\n",
      "     Depress       0.00      0.00      0.00        99\n",
      "\n",
      "    accuracy                           0.77       439\n",
      "   macro avg       0.39      0.50      0.44       439\n",
      "weighted avg       0.60      0.77      0.68       439\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "def create_dl_model():\n",
    "    num_filters = 250\n",
    "    drop = 0.2\n",
    "\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                input_length=INPUT_LENGTH,\n",
    "                                weights=[embedding_matrix],\n",
    "                                trainable=False)\n",
    "                                # trainable=True)\n",
    "\n",
    "#     model_input = Input(shape=(15000,), dtype='int32')\n",
    "#     model_layer = embedding_layer(model_input)\n",
    "#     model_layer = Dropout(drop)(model_layer)\n",
    "#     model_layer = Conv1D(filters=num_filters, kernel_size=3)(model_layer)\n",
    "#     model_layer = GlobalMaxPooling1D()(model_layer)\n",
    "#     # model_layer = Dropout(drop)(model_layer)\n",
    "#     model_layer = Dense(num_filters, activation='relu')(model_layer)\n",
    "#     model_layer = Dropout(drop)(model_layer)\n",
    "#     model_layer = Dense(1, activation='sigmoid')(model_layer)\n",
    "#     model = Model(model_input, model_layer)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words,\n",
    "                        EMBEDDING_DIM,\n",
    "                        input_length=15000,\n",
    "                        weights=[embedding_matrix],\n",
    "                                trainable=False))\n",
    "#                         trainable=True))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Conv1D(filters=num_filters, kernel_size=3))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(250))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(drop))\n",
    "#     model.add(Dense(1))\n",
    "#     model.add(Activation('sigmoid'))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    print(model.summary())\n",
    "    adam = Adam(lr=0.004, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    # adam = AdaMod()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])  # metrics.binary_accuracy\n",
    "    return model\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "K.set_session(sess)\n",
    "\n",
    "class_weight = get_class_weights(np.asarray(y_train, 'int32')[:, 1])\n",
    "\n",
    "FOLDER = \"{}/{}\".format(MODEL_FOLDER, datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M\"))\n",
    "if not os.path.exists(FOLDER):\n",
    "    os.makedirs(FOLDER)\n",
    "callbacks = TensorBoard(log_dir=FOLDER,\n",
    "                        write_graph=True, write_grads=False, histogram_freq=0,\n",
    "                        write_images=True, embeddings_freq=0, embeddings_layer_names='embedding_1',\n",
    "                        embeddings_metadata=None)\n",
    "\n",
    "checkpoint = ModelCheckpoint(FOLDER + '/model.{epoch:02d}-{val_acc:.2f}.hdf5',\n",
    "                             verbose=0, monitor='val_acc',\n",
    "                             save_best_only=True, mode='auto')\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='auto', verbose=0, patience=10)\n",
    "\n",
    "print(\"model fitting - Hierachical LSTM\")\n",
    "\n",
    "print(class_weight)\n",
    "model = create_dl_model()\n",
    "\n",
    "X_train2 = np.zeros((len(X_train), 15000), dtype='int32')\n",
    "for i, posts in enumerate(X_train):\n",
    "    seq_data = pad_sequences([posts], maxlen=15000, padding='pre', truncating='post')\n",
    "    X_train2[i] = seq_data\n",
    "    \n",
    "x_val2 = np.zeros((len(x_val), 15000), dtype='int32')\n",
    "for i, posts in enumerate(x_val):\n",
    "    seq_data = pad_sequences([posts], maxlen=15000, padding='pre', truncating='post')\n",
    "    x_val2[i] = seq_data\n",
    "\n",
    "model.fit(X_train2, np.asarray(y_train, 'int32'),\n",
    "# model.fit(X_train2, np.asarray(y_train[:, 1], 'int32'),\n",
    "          validation_data=(x_val2, np.asarray(y_val, 'int32')),\n",
    "#           validation_data=(x_val2, np.asarray(y_val[:, 1], 'int32')),\n",
    "          shuffle=False, class_weight=class_weight,\n",
    "          nb_epoch=50, batch_size=32, verbose=0, \n",
    "          callbacks=[checkpoint, callbacks])\n",
    "\n",
    "model.save(FOLDER + '/my_model.h5')\n",
    "\n",
    "\n",
    "\n",
    "print('--------------Start eva----------------'.format(datetime.datetime.now()))\n",
    "model_name = sorted(glob.iglob(os.path.join(FOLDER, '*.h5')), key=os.path.getctime, reverse=True)[0]\n",
    "model = load_model(model_name)\n",
    "probas_ = model.predict(x_val2)\n",
    "y_test = y_val[:, 1].astype(int)\n",
    "max_accuracy, thresh = get_max_acc(y_test, probas_[:, 1])\n",
    "pred = [1 if m > thresh else 0 for m in probas_[:, 1]]\n",
    "# max_accuracy, thresh = get_max_acc(y_test, probas_)\n",
    "# pred = [1 if m > thresh else 0 for m in probas_]\n",
    "print('Fold {} accuracy = {:.2f}'.format(fold + 1, max_accuracy * 100))\n",
    "print(classification_report(y_test, pred, target_names=['Control', 'Depress']))\n",
    "\n",
    "df = pd.DataFrame(y_test, columns=['label'])\n",
    "#         df['class0'] = probas_[:, 0]\n",
    "df['class1'] = probas_[:, 1]\n",
    "df.to_csv(MODEL_FOLDER + '/dl-' + str(INPUT_LENGTH) + '-' + str(EMBEDDING_DIM) + str(fold) + '.csv', index=False)\n",
    "\n",
    "weighted = precision_recall_fscore_support(y_test, pred, average='weighted')\n",
    "macro = precision_recall_fscore_support(y_test, pred, average='macro')\n",
    "micro = precision_recall_fscore_support(y_test, pred, average='micro')\n",
    "both = precision_recall_fscore_support(y_test, pred)\n",
    "results.append(['dl-max-' + str(fold),\n",
    "                max_accuracy, weighted[0], weighted[1], weighted[2],\n",
    "                macro[0], macro[1], macro[2], micro[0], micro[1], micro[2],\n",
    "                both[0][0], both[1][0], both[2][0], both[0][1], both[1][1], both[2][1]])\n",
    "\n",
    "pred = [1 if m > 0.5 else 0 for m in probas_[:, 1]]\n",
    "print('Fold {} normal accuracy = {:.2f}'.format(fold + 1, accuracy_score(y_test, pred) * 100))\n",
    "max_accuracy = accuracy_score(y_test, pred)\n",
    "print(classification_report(y_test, pred, target_names=['Control', 'Depress']))\n",
    "\n",
    "weighted = precision_recall_fscore_support(y_test, pred, average='weighted')\n",
    "macro = precision_recall_fscore_support(y_test, pred, average='macro')\n",
    "micro = precision_recall_fscore_support(y_test, pred, average='micro')\n",
    "both = precision_recall_fscore_support(y_test, pred)\n",
    "results.append(['dl-nor-' + str(fold),\n",
    "                max_accuracy, weighted[0], weighted[1], weighted[2],\n",
    "                macro[0], macro[1], macro[2], micro[0], micro[1], micro[2],\n",
    "                both[0][0], both[1][0], both[2][0], both[0][1], both[1][1], both[2][1]])\n",
    "\n",
    "print('-------------------------------------------Start Best Model--------------------------------------------')\n",
    "\n",
    "model_name = sorted(glob.iglob(os.path.join(FOLDER, '*.hdf5')), key=os.path.getctime, reverse=True)[0]\n",
    "model = load_model(model_name)\n",
    "probas_ = model.predict(x_val2)\n",
    "y_test = y_val[:, 1].astype(int)\n",
    "max_accuracy, thresh = get_max_acc(y_test, probas_[:, 1])\n",
    "pred = [1 if m > thresh else 0 for m in probas_[:, 1]]\n",
    "# max_accuracy, thresh = get_max_acc(y_test, probas_)\n",
    "# pred = [1 if m > thresh else 0 for m in probas_]\n",
    "print('Fold {} accuracy = {:.2f}'.format(fold + 1, max_accuracy * 100))\n",
    "print(classification_report(y_test, pred, target_names=['Control', 'Depress']))\n",
    "\n",
    "df = pd.DataFrame(y_test, columns=['label'])\n",
    "#         df['class0'] = probas_[:, 0]\n",
    "df['class1'] = probas_[:, 1]\n",
    "df.to_csv(MODEL_FOLDER + '/dl-best-' + str(INPUT_LENGTH) + '-' + str(EMBEDDING_DIM) + str(fold) + '.csv', index=False)\n",
    "\n",
    "weighted = precision_recall_fscore_support(y_test, pred, average='weighted')\n",
    "macro = precision_recall_fscore_support(y_test, pred, average='macro')\n",
    "micro = precision_recall_fscore_support(y_test, pred, average='micro')\n",
    "both = precision_recall_fscore_support(y_test, pred)\n",
    "results.append(['dl-best-max-' + str(fold),\n",
    "                max_accuracy, weighted[0], weighted[1], weighted[2],\n",
    "                macro[0], macro[1], macro[2], micro[0], micro[1], micro[2],\n",
    "                both[0][0], both[1][0], both[2][0], both[0][1], both[1][1], both[2][1]])\n",
    "\n",
    "pred = [1 if m > 0.5 else 0 for m in probas_[:, 1]]\n",
    "print('Fold {} normal accuracy = {:.2f}'.format(fold + 1, accuracy_score(y_test, pred) * 100))\n",
    "max_accuracy = accuracy_score(y_test, pred)\n",
    "print(classification_report(y_test, pred, target_names=['Control', 'Depress']))\n",
    "\n",
    "weighted = precision_recall_fscore_support(y_test, pred, average='weighted')\n",
    "macro = precision_recall_fscore_support(y_test, pred, average='macro')\n",
    "micro = precision_recall_fscore_support(y_test, pred, average='micro')\n",
    "both = precision_recall_fscore_support(y_test, pred)\n",
    "results.append(['dl-best-nor-' + str(fold),\n",
    "                max_accuracy, weighted[0], weighted[1], weighted[2],\n",
    "                macro[0], macro[1], macro[2], micro[0], micro[1], micro[2],\n",
    "                both[0][0], both[1][0], both[2][0], both[0][1], both[1][1], both[2][1]])\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Hierachical LSTM\n",
      "{0: 1.0, 1: 3.4231738035264483}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_25 (InputLayer)        (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "embedding_25 (Embedding)     (None, 10000, 100)        5000000   \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 10000, 100)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 9998, 250)         75250     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_24 (Glo (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 5,138,251\n",
      "Trainable params: 5,138,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bright\\.conda\\envs\\py_37\\lib\\site-packages\\ipykernel_launcher.py:71: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Start eva----------------\n",
      "Fold 1 accuracy = 85.88\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Control       0.86      0.97      0.91       340\n",
      "     Depress       0.82      0.47      0.60        99\n",
      "\n",
      "    accuracy                           0.86       439\n",
      "   macro avg       0.84      0.72      0.76       439\n",
      "weighted avg       0.86      0.86      0.84       439\n",
      "\n",
      "Fold 1 normal accuracy = 83.14\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Control       0.90      0.89      0.89       340\n",
      "     Depress       0.62      0.65      0.63        99\n",
      "\n",
      "    accuracy                           0.83       439\n",
      "   macro avg       0.76      0.77      0.76       439\n",
      "weighted avg       0.83      0.83      0.83       439\n",
      "\n",
      "-------------------------------------------Start Best Model--------------------------------------------\n",
      "Fold 1 accuracy = 86.79\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Control       0.88      0.96      0.92       340\n",
      "     Depress       0.80      0.56      0.65        99\n",
      "\n",
      "    accuracy                           0.87       439\n",
      "   macro avg       0.84      0.76      0.79       439\n",
      "weighted avg       0.86      0.87      0.86       439\n",
      "\n",
      "Fold 1 normal accuracy = 87.02\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Control       0.88      0.96      0.92       340\n",
      "     Depress       0.80      0.57      0.66        99\n",
      "\n",
      "    accuracy                           0.87       439\n",
      "   macro avg       0.84      0.76      0.79       439\n",
      "weighted avg       0.86      0.87      0.86       439\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_dl_model():\n",
    "    num_filters = 250\n",
    "    drop = 0.3\n",
    "\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                input_length=10000,\n",
    "#                                 weights=[embedding_matrix],\n",
    "#                                 trainable=False)\n",
    "                                trainable=True)\n",
    "\n",
    "    model_input = Input(shape=(10000,), dtype='int32')\n",
    "    model_layer = embedding_layer(model_input)\n",
    "    model_layer = Dropout(drop)(model_layer)\n",
    "    model_layer = Conv1D(filters=num_filters, kernel_size=3)(model_layer)\n",
    "    model_layer = GlobalMaxPooling1D()(model_layer)\n",
    "    # model_layer = Dropout(drop)(model_layer)\n",
    "    model_layer = Dense(num_filters, activation='relu')(model_layer)\n",
    "    model_layer = Dropout(drop)(model_layer)\n",
    "#     model_layer = Dense(2, activation='softmax')(model_layer)\n",
    "    model_layer = Dense(1, activation='sigmoid')(model_layer)\n",
    "    model = Model(model_input, model_layer)\n",
    "\n",
    "    print(model.summary())\n",
    "    adam = Adam(lr=0.005, clipnorm=7.0)\n",
    "    # adam = AdaMod()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])  # metrics.binary_accuracy\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "class_weight = get_class_weights(np.asarray(y_train, 'int32')[:, 1])\n",
    "\n",
    "FOLDER = \"{}/{}\".format(MODEL_FOLDER, datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M\"))\n",
    "if not os.path.exists(FOLDER):\n",
    "    os.makedirs(FOLDER)\n",
    "callbacks = TensorBoard(log_dir=FOLDER,\n",
    "                        write_graph=True, write_grads=False, histogram_freq=0,\n",
    "                        write_images=True, embeddings_freq=0, embeddings_layer_names='embedding_1',\n",
    "                        embeddings_metadata=None)\n",
    "\n",
    "checkpoint = ModelCheckpoint(FOLDER + '/model.{epoch:02d}-{val_acc:.2f}.hdf5',\n",
    "                             verbose=0, monitor='val_acc',\n",
    "                             save_best_only=True, mode='auto')\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='auto', verbose=0, patience=10)\n",
    "\n",
    "print(\"model fitting - Hierachical LSTM\")\n",
    "\n",
    "print(class_weight)\n",
    "model = create_dl_model()\n",
    "\n",
    "X_train2 = np.zeros((len(X_train), 10000), dtype='int32')\n",
    "for i, posts in enumerate(X_train):\n",
    "    seq_data = pad_sequences([posts], maxlen=10000, padding='pre', truncating='post')\n",
    "    X_train2[i] = seq_data\n",
    "    \n",
    "x_val2 = np.zeros((len(x_val), 10000), dtype='int32')\n",
    "for i, posts in enumerate(x_val):\n",
    "    seq_data = pad_sequences([posts], maxlen=10000, padding='pre', truncating='post')\n",
    "    x_val2[i] = seq_data\n",
    "\n",
    "# model.fit(X_train, np.asarray(y_train, 'int32'),\n",
    "model.fit(X_train2, np.asarray(y_train[:, 1], 'int32'),\n",
    "#           validation_data=(x_val, np.asarray(y_val, 'int32')),\n",
    "          validation_data=(x_val2, np.asarray(y_val[:, 1], 'int32')),\n",
    "          shuffle=False, \n",
    "          nb_epoch=200, batch_size=32, verbose=0, \n",
    "          callbacks=[checkpoint, callbacks, es])\n",
    "\n",
    "model.save(FOLDER + '/my_model.h5')\n",
    "\n",
    "\n",
    "\n",
    "print('--------------Start eva----------------'.format(datetime.datetime.now()))\n",
    "model_name = sorted(glob.iglob(os.path.join(FOLDER, '*.h5')), key=os.path.getctime, reverse=True)[0]\n",
    "model = load_model(model_name,\n",
    "                   custom_objects={'optimizer': AdaMod})\n",
    "probas_ = model.predict(x_val2)\n",
    "y_test = y_val[:, 1].astype(int)\n",
    "#         max_accuracy, thresh = get_max_acc(y_test, probas_[:, 1])\n",
    "#         pred = [1 if m > thresh else 0 for m in probas_[:, 1]]\n",
    "max_accuracy, thresh = get_max_acc(y_test, probas_)\n",
    "pred = [1 if m > thresh else 0 for m in probas_]\n",
    "print('Fold {} accuracy = {:.2f}'.format(fold + 1, max_accuracy * 100))\n",
    "print(classification_report(y_test, pred, target_names=['Control', 'Depress']))\n",
    "\n",
    "df = pd.DataFrame(y_test, columns=['label'])\n",
    "#         df['class0'] = probas_[:, 0]\n",
    "df['class1'] = probas_\n",
    "df.to_csv(MODEL_FOLDER + '/dl-' + str(INPUT_LENGTH) + '-' + str(EMBEDDING_DIM) + str(fold) + '.csv', index=False)\n",
    "\n",
    "weighted = precision_recall_fscore_support(y_test, pred, average='weighted')\n",
    "macro = precision_recall_fscore_support(y_test, pred, average='macro')\n",
    "micro = precision_recall_fscore_support(y_test, pred, average='micro')\n",
    "both = precision_recall_fscore_support(y_test, pred)\n",
    "results.append(['dl-max-' + str(fold),\n",
    "                max_accuracy, weighted[0], weighted[1], weighted[2],\n",
    "                macro[0], macro[1], macro[2], micro[0], micro[1], micro[2],\n",
    "                both[0][0], both[1][0], both[2][0], both[0][1], both[1][1], both[2][1]])\n",
    "\n",
    "pred = [1 if m > 0.5 else 0 for m in probas_]\n",
    "print('Fold {} normal accuracy = {:.2f}'.format(fold + 1, accuracy_score(y_test, pred) * 100))\n",
    "max_accuracy = accuracy_score(y_test, pred)\n",
    "print(classification_report(y_test, pred, target_names=['Control', 'Depress']))\n",
    "\n",
    "weighted = precision_recall_fscore_support(y_test, pred, average='weighted')\n",
    "macro = precision_recall_fscore_support(y_test, pred, average='macro')\n",
    "micro = precision_recall_fscore_support(y_test, pred, average='micro')\n",
    "both = precision_recall_fscore_support(y_test, pred)\n",
    "results.append(['dl-nor-' + str(fold),\n",
    "                max_accuracy, weighted[0], weighted[1], weighted[2],\n",
    "                macro[0], macro[1], macro[2], micro[0], micro[1], micro[2],\n",
    "                both[0][0], both[1][0], both[2][0], both[0][1], both[1][1], both[2][1]])\n",
    "\n",
    "print('-------------------------------------------Start Best Model--------------------------------------------')\n",
    "\n",
    "model_name = sorted(glob.iglob(os.path.join(FOLDER, '*.hdf5')), key=os.path.getctime, reverse=True)[0]\n",
    "model = load_model(model_name,\n",
    "                   custom_objects={'optimizer': AdaMod})\n",
    "probas_ = model.predict(x_val2)\n",
    "y_test = y_val[:, 1].astype(int)\n",
    "#         max_accuracy, thresh = get_max_acc(y_test, probas_[:, 1])\n",
    "#         pred = [1 if m > thresh else 0 for m in probas_[:, 1]]\n",
    "max_accuracy, thresh = get_max_acc(y_test, probas_)\n",
    "pred = [1 if m > thresh else 0 for m in probas_]\n",
    "print('Fold {} accuracy = {:.2f}'.format(fold + 1, max_accuracy * 100))\n",
    "print(classification_report(y_test, pred, target_names=['Control', 'Depress']))\n",
    "\n",
    "df = pd.DataFrame(y_test, columns=['label'])\n",
    "#         df['class0'] = probas_[:, 0]\n",
    "df['class1'] = probas_\n",
    "df.to_csv(MODEL_FOLDER + '/dl-best-' + str(INPUT_LENGTH) + '-' + str(EMBEDDING_DIM) + str(fold) + '.csv', index=False)\n",
    "\n",
    "weighted = precision_recall_fscore_support(y_test, pred, average='weighted')\n",
    "macro = precision_recall_fscore_support(y_test, pred, average='macro')\n",
    "micro = precision_recall_fscore_support(y_test, pred, average='micro')\n",
    "both = precision_recall_fscore_support(y_test, pred)\n",
    "results.append(['dl-best-max-' + str(fold),\n",
    "                max_accuracy, weighted[0], weighted[1], weighted[2],\n",
    "                macro[0], macro[1], macro[2], micro[0], micro[1], micro[2],\n",
    "                both[0][0], both[1][0], both[2][0], both[0][1], both[1][1], both[2][1]])\n",
    "\n",
    "pred = [1 if m > 0.5 else 0 for m in probas_]\n",
    "print('Fold {} normal accuracy = {:.2f}'.format(fold + 1, accuracy_score(y_test, pred) * 100))\n",
    "max_accuracy = accuracy_score(y_test, pred)\n",
    "print(classification_report(y_test, pred, target_names=['Control', 'Depress']))\n",
    "\n",
    "weighted = precision_recall_fscore_support(y_test, pred, average='weighted')\n",
    "macro = precision_recall_fscore_support(y_test, pred, average='macro')\n",
    "micro = precision_recall_fscore_support(y_test, pred, average='micro')\n",
    "both = precision_recall_fscore_support(y_test, pred)\n",
    "results.append(['dl-best-nor-' + str(fold),\n",
    "                max_accuracy, weighted[0], weighted[1], weighted[2],\n",
    "                macro[0], macro[1], macro[2], micro[0], micro[1], micro[2],\n",
    "                both[0][0], both[1][0], both[2][0], both[0][1], both[1][1], both[2][1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
